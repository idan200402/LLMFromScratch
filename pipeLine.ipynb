{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T22:31:33.739182Z",
     "start_time": "2025-12-01T22:31:33.735756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('input.txt', 'r' , encoding='utf-8') as file:\n",
    "    data = file.read()"
   ],
   "id": "c5d25d0c3060060",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T22:24:51.178950Z",
     "start_time": "2025-12-01T22:24:51.176590Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"the legth is :\", len(data))",
   "id": "5ebe983eaa2a0c50",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the legth is : 1115394\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T22:24:51.199118Z",
     "start_time": "2025-12-01T22:24:51.190954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chars = sorted(list(set(data)))\n",
    "print(\"the unique chars are :\", ''.join(chars))\n",
    "print(\"the number of vocabulary is :\", len(chars))"
   ],
   "id": "5db3611893a58c0a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the unique chars are : \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "the number of vocabulary is : 65\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T22:24:51.208388Z",
     "start_time": "2025-12-01T22:24:51.206128Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#maps from char to int\n",
    "char_to_int = {ch:i for i,ch in enumerate(chars)}\n",
    "#maps from int to char\n",
    "int_to_char = {i:ch for i,ch in enumerate(chars)}\n",
    "#function to encode a string to a list of integers\n",
    "encode = lambda s: [char_to_int[c] for c in s]\n",
    "#function to decode a list of integers to a string\n",
    "decode = lambda l: ''.join([int_to_char[i] for i in l])\n"
   ],
   "id": "eeef2977f9e1463b",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T22:24:51.221392Z",
     "start_time": "2025-12-01T22:24:51.218394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#example usage\n",
    "enc = encode(\"mihmoret\")\n",
    "print(enc)\n",
    "dec = decode(enc)\n",
    "print(dec)\n",
    "#example of handling character not in the vocabulary\n",
    "try:\n",
    "    enc2 = encode(\"+\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error: {e} is not in the character set.\")"
   ],
   "id": "1954fe9e088905e1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51, 47, 46, 51, 53, 56, 43, 58]\n",
      "mihmoret\n",
      "Error: '+' is not in the character set.\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T22:24:51.307727Z",
     "start_time": "2025-12-01T22:24:51.231636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"using device:\", device)\n",
    "\n",
    "text = data\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long, device=device)\n",
    "print(data.shape, data.dtype, data.device)\n",
    "print(data[:100])\n"
   ],
   "id": "1fa3c60baa39aa9b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "torch.Size([1115394]) torch.int64 cuda:0\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59], device='cuda:0')\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T22:24:51.314731Z",
     "start_time": "2025-12-01T22:24:51.311732Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#split the data into train and validation sets\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ],
   "id": "dab3c18f3bfd020",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T22:24:51.326064Z",
     "start_time": "2025-12-01T22:24:51.323752Z"
    }
   },
   "cell_type": "code",
   "source": "block_size = 8",
   "id": "a58d4de97f911da2",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T22:24:51.335457Z",
     "start_time": "2025-12-01T22:24:51.331070Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#example 0: demonstrate how to create input and target sequences\n",
    "#producing input and target sequences (labels)\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "#for\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context.tolist()} the target is {target.item()}\")"
   ],
   "id": "9d84f9a2bf4ed319",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is [18] the target is 47\n",
      "when input is [18, 47] the target is 56\n",
      "when input is [18, 47, 56] the target is 57\n",
      "when input is [18, 47, 56, 57] the target is 58\n",
      "when input is [18, 47, 56, 57, 58] the target is 1\n",
      "when input is [18, 47, 56, 57, 58, 1] the target is 15\n",
      "when input is [18, 47, 56, 57, 58, 1, 15] the target is 47\n",
      "when input is [18, 47, 56, 57, 58, 1, 15, 47] the target is 58\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T22:24:51.356526Z",
     "start_time": "2025-12-01T22:24:51.346463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4  #how many independent sequences will we process in parallel\n",
    "block_size = 8  #what is the maximum context length for predictions\n",
    "def get_batch(split):\n",
    "    #generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # 4 random starting indices along the data\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) # 4 sequences of length block_size , 4X8 tensor.\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # 4 sequences of length block_size , 4X8 tensor.the labels.\n",
    "    return x, y\n",
    "\n",
    "xb , yb = get_batch('train')\n",
    "xb.to('cuda')\n",
    "yb.to('cuda')\n",
    "print(\"inputs:\")\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print(\"outputs:\")\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "print(\"---\")\n",
    "\n",
    "#example of input and target.\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b,:t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target is {target.item()}\")"
   ],
   "id": "5692cf83aff222d5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]], device='cuda:0')\n",
      "outputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]], device='cuda:0')\n",
      "---\n",
      "when input is [24] the target is 43\n",
      "when input is [24, 43] the target is 58\n",
      "when input is [24, 43, 58] the target is 5\n",
      "when input is [24, 43, 58, 5] the target is 57\n",
      "when input is [24, 43, 58, 5, 57] the target is 1\n",
      "when input is [24, 43, 58, 5, 57, 1] the target is 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] the target is 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target is 39\n",
      "when input is [44] the target is 53\n",
      "when input is [44, 53] the target is 56\n",
      "when input is [44, 53, 56] the target is 1\n",
      "when input is [44, 53, 56, 1] the target is 58\n",
      "when input is [44, 53, 56, 1, 58] the target is 46\n",
      "when input is [44, 53, 56, 1, 58, 46] the target is 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] the target is 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target is 1\n",
      "when input is [52] the target is 58\n",
      "when input is [52, 58] the target is 1\n",
      "when input is [52, 58, 1] the target is 58\n",
      "when input is [52, 58, 1, 58] the target is 46\n",
      "when input is [52, 58, 1, 58, 46] the target is 39\n",
      "when input is [52, 58, 1, 58, 46, 39] the target is 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] the target is 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target is 46\n",
      "when input is [25] the target is 17\n",
      "when input is [25, 17] the target is 27\n",
      "when input is [25, 17, 27] the target is 10\n",
      "when input is [25, 17, 27, 10] the target is 0\n",
      "when input is [25, 17, 27, 10, 0] the target is 21\n",
      "when input is [25, 17, 27, 10, 0, 21] the target is 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] the target is 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target is 39\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Creating the bigram language model And training it.",
   "id": "ee8f310d789beced"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T22:24:51.390530Z",
     "start_time": "2025-12-01T22:24:51.363530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#bigram model\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        #each row represents a token id , and each mat[i,j] is the score for jth token in the vocab given ith token.\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    def forward(self, idx, targets=None):\n",
    "        #idx is the BXT tensor of input token indices .\n",
    "        logits = self.token_embedding_table(idx) #BXTXC.\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B , T, C = logits.shape\n",
    "            logits = logits.view(B*T , C) # (BXT)XC. reshaping for loss computation 3d to 2d.\n",
    "            targets = targets.view(B*T) # (BXT) . reshaping for loss computation 2d to 1d.\n",
    "            #how well we are predicting the targets\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            #for each entry we will return the logits for the next token.\n",
    "        return logits , loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        #idx is a BXT tensor of input token\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits , loss = self(idx)\n",
    "            logits = logits[:, -1, :] #BXC  , for each batch we take the predictions for the last token.\n",
    "            probs = F.softmax(logits, dim=-1) #BXC , converting to probabilities\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) #BX1 , the next predicted token for each batch.\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # BX(T+1) , appending the predicted token to the input sequence.\n",
    "        return idx\n",
    "\n",
    "\n",
    "m = BigramLanguageModel(vocab_size=len(chars)).to(device)\n",
    "out , loss = m(xb , yb)\n",
    "print(out)\n",
    "print(loss)\n",
    "\n",
    "\n",
    "# a tensor with a single zero , B=1 , T=1 , representing the start token.\n",
    "idx = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "#generating 100 new tokens\n",
    "print(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))\n"
   ],
   "id": "65dca01a0850bff7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5101, -0.0948,  1.0927,  ..., -0.6126, -0.6597,  0.7624],\n",
      "        [ 0.3323, -0.0872, -0.7470,  ..., -0.6716, -0.9572, -0.9594],\n",
      "        [ 0.2475, -0.6349, -1.2909,  ...,  1.3064, -0.2256, -1.8305],\n",
      "        ...,\n",
      "        [-2.1910, -0.7574,  1.9656,  ..., -0.3580,  0.8585, -0.6161],\n",
      "        [ 0.5978, -0.0514, -0.0646,  ..., -1.4649, -2.0555,  1.8275],\n",
      "        [-0.6787,  0.8662, -1.6433,  ...,  2.3671, -0.7775, -0.2586]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "tensor(4.8786, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "\n",
      "pYCXxfRkRZd\n",
      "wc'wfNfT;OLlTEeC K\n",
      "jxqPToTb?bXAUG:C-SGJO-33SM:C?YI3a\n",
      "hs:LVXJFhXeNuwqhObxZ.tSVrddXlaSZaNe\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T22:24:51.397643Z",
     "start_time": "2025-12-01T22:24:51.395582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#training the model , defining the optimizer.\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ],
   "id": "d8f563dacc35c592",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T22:24:52.330886Z",
     "start_time": "2025-12-01T22:24:51.410648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 32 #how many independent sequences will we process in parallel\n",
    "for steps in range(1000):\n",
    "    #get a batch of data\n",
    "    xb , yb = get_batch('train')\n",
    "    #evaluate the loss\n",
    "    logits , loss = m(xb, yb)\n",
    "    #backpropagation\n",
    "    optimizer.zero_grad(set_to_none=True) #set gradients to zero , to avoid accumulation from previous step.\n",
    "    loss.backward() # compute gradients , see how much each weight contributed to the loss.\n",
    "    optimizer.step()\n",
    "    if steps % 10 == 0:\n",
    "        print(steps, loss.item())"
   ],
   "id": "bad5917a434639c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.648484230041504\n",
      "10 4.74024772644043\n",
      "20 4.635769367218018\n",
      "30 4.7977776527404785\n",
      "40 4.6954665184021\n",
      "50 4.581179618835449\n",
      "60 4.682138919830322\n",
      "70 4.519830703735352\n",
      "80 4.635293006896973\n",
      "90 4.661656856536865\n",
      "100 4.642975807189941\n",
      "110 4.68632698059082\n",
      "120 4.497181415557861\n",
      "130 4.556583881378174\n",
      "140 4.6776275634765625\n",
      "150 4.449394226074219\n",
      "160 4.50913143157959\n",
      "170 4.5243000984191895\n",
      "180 4.548206806182861\n",
      "190 4.4104084968566895\n",
      "200 4.47345495223999\n",
      "210 4.459536075592041\n",
      "220 4.4392805099487305\n",
      "230 4.460430145263672\n",
      "240 4.4244866371154785\n",
      "250 4.42477560043335\n",
      "260 4.36106014251709\n",
      "270 4.415848731994629\n",
      "280 4.46035099029541\n",
      "290 4.366547107696533\n",
      "300 4.2544732093811035\n",
      "310 4.222301483154297\n",
      "320 4.382016658782959\n",
      "330 4.376062870025635\n",
      "340 4.3820414543151855\n",
      "350 4.366427898406982\n",
      "360 4.201217174530029\n",
      "370 4.353005886077881\n",
      "380 4.281277656555176\n",
      "390 4.290494918823242\n",
      "400 4.29338264465332\n",
      "410 4.132618427276611\n",
      "420 4.231373310089111\n",
      "430 4.197425365447998\n",
      "440 4.257749557495117\n",
      "450 4.179753303527832\n",
      "460 4.276759147644043\n",
      "470 4.148818492889404\n",
      "480 4.204633712768555\n",
      "490 4.236260414123535\n",
      "500 4.162595748901367\n",
      "510 4.14796781539917\n",
      "520 4.220947742462158\n",
      "530 4.178046703338623\n",
      "540 4.100080490112305\n",
      "550 4.201590538024902\n",
      "560 4.042474746704102\n",
      "570 3.9905407428741455\n",
      "580 4.056334018707275\n",
      "590 4.065803527832031\n",
      "600 4.0203423500061035\n",
      "610 4.006284236907959\n",
      "620 4.049383640289307\n",
      "630 4.1103739738464355\n",
      "640 4.08767557144165\n",
      "650 3.985372304916382\n",
      "660 3.986151695251465\n",
      "670 4.013110160827637\n",
      "680 4.0221686363220215\n",
      "690 3.9954051971435547\n",
      "700 4.030514240264893\n",
      "710 4.110603332519531\n",
      "720 3.9340009689331055\n",
      "730 3.965081214904785\n",
      "740 3.85903263092041\n",
      "750 3.925828218460083\n",
      "760 3.941173553466797\n",
      "770 3.9219741821289062\n",
      "780 3.911132574081421\n",
      "790 3.8220386505126953\n",
      "800 3.8753600120544434\n",
      "810 3.831495761871338\n",
      "820 3.937608480453491\n",
      "830 4.014145374298096\n",
      "840 3.8120458126068115\n",
      "850 3.8790812492370605\n",
      "860 3.7466893196105957\n",
      "870 3.8935012817382812\n",
      "880 3.770812749862671\n",
      "890 3.7455272674560547\n",
      "900 3.810453414916992\n",
      "910 3.8390934467315674\n",
      "920 3.837909698486328\n",
      "930 3.809741258621216\n",
      "940 3.7322657108306885\n",
      "950 3.743919849395752\n",
      "960 3.765810012817383\n",
      "970 3.795292854309082\n",
      "980 3.77083683013916\n",
      "990 3.720672130584717\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T22:24:52.433228Z",
     "start_time": "2025-12-01T22:24:52.370935Z"
    }
   },
   "cell_type": "code",
   "source": [
    " #geenrating a  1 degree  tensor of predicted tokens , then decoding and printing them.\n",
    " print(decode(m.generate(idx, max_new_tokens=300)[0].tolist()))\n",
    " out , loss = m(xb , yb)\n",
    " print(out)\n",
    " print(loss)"
   ],
   "id": "675eb969be52677a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "W?w3cHPyZWk,f's$a-oizCjmuX\n",
      "YoR&$FMVofXisEvB!!BA!$W:CdYlixcaeg ireeYERnkcin;lxWiHFliqmoGSKtSV&BLqWk p.SGFo.\n",
      "SGjbo!UelIlind,pea!.\n",
      "-huD3SPyckzby:CUup;MOissX3Qwty.OJlvBPUSIkyBf&patelgCIEJMk:Chll,SPlyltSPkqmoRW-wNAXQbjxCevib3sr'T:C-&dE$HZAETENehhir$Fstp-LK3:CJ-xTrg\n",
      "\n",
      "ALkOdmnunruf?qA so;;3QQkhWTE:CEEwfep$v\n",
      "tensor([[ 0.3500,  0.8383, -2.9546,  ..., -1.4349, -0.3560, -0.1226],\n",
      "        [-0.6300,  0.2204, -0.5048,  ..., -1.4783, -1.4049, -1.2824],\n",
      "        [-0.7509, -0.7660, -0.1990,  ...,  1.3423,  1.9044, -0.2119],\n",
      "        ...,\n",
      "        [ 0.2930,  0.8083, -0.3381,  ..., -1.4258, -1.4286, -0.6515],\n",
      "        [ 0.3926,  0.8269, -0.7454,  ..., -0.7713, -0.6833, -1.7850],\n",
      "        [-0.3034, -0.9346, -0.9612,  ..., -2.3500, -1.3458,  0.9166]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "tensor(3.8054, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A toy example of how tokens refer to each other during self attention.",
   "id": "54e83475f4a28cac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T22:24:52.443380Z",
     "start_time": "2025-12-01T22:24:52.438233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(1337)\n",
    "B , T, C = 4 , 8 , 2 #batch size , time steps , channels\n",
    "x = torch.randn(B, T, C).to(device)\n",
    "x.shape"
   ],
   "id": "c6462b1b4eaa1694",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The goal  , make the token in time t hold semantic information about all previous tokens from time 0 to t.\n",
    "First attempt , with nested for loops."
   ],
   "id": "8999ca6adbe3dc9b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T22:24:52.510615Z",
     "start_time": "2025-12-01T22:24:52.453385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "xbow = torch.zeros((B, T, C), device=device) # bow is \"bag of words\"\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1] #t+1 because we want to include the current time step  . TXC\n",
    "        xbow[b, t] = torch.mean(xprev, dim=0) #average"
   ],
   "id": "d9207b50085b153c",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T22:24:52.521884Z",
     "start_time": "2025-12-01T22:24:52.515862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"the tokens for batch 0:\"  , x[0])\n",
    "print(\"enriched tokens for batch 0 :\" , xbow[0])"
   ],
   "id": "348819e187af7949",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the tokens for batch 0: tensor([[ 0.1808, -0.0700],\n",
      "        [-0.3596, -0.9152],\n",
      "        [ 0.6258,  0.0255],\n",
      "        [ 0.9545,  0.0643],\n",
      "        [ 0.3612,  1.1679],\n",
      "        [-1.3499, -0.5102],\n",
      "        [ 0.2360, -0.2398],\n",
      "        [-0.9211,  1.5433]], device='cuda:0')\n",
      "enriched tokens for batch 0 : tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]], device='cuda:0')\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Second attempt , using matrix multiplication.",
   "id": "505fc40dd7348efd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T22:24:52.631798Z",
     "start_time": "2025-12-01T22:24:52.529889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3,3)).to(device)\n",
    "a = a / torch.sum(a, dim=1, keepdim=True) #normalize so that each row sums to 1\n",
    "b = torch.randint(0,10,(3,2)).float().to(device)\n",
    "c = a @ b\n",
    "print(\"a:\")\n",
    "print(a)\n",
    "print(\"b:\")\n",
    "print(b)\n",
    "print(\"c:\")\n",
    "print(c)"
   ],
   "id": "5ca2d10d9d6a5bc9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]], device='cuda:0')\n",
      "b:\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]], device='cuda:0')\n",
      "c:\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]], device='cuda:0')\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T22:24:52.776240Z",
     "start_time": "2025-12-01T22:24:52.639817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wei = torch.tril(torch.ones(T, T)).to(device)\n",
    "wei = wei / torch.sum(wei, dim=1, keepdim=True)\n",
    "xbow2 = wei @ x  # TXT X BXTXC ---> BXTXT X BXTXC --> BXTXC\n",
    "b = 1\n",
    "\n",
    "torch.allclose(xbow, xbow2 , atol=1e-6 , rtol=1e-5)\n"
   ],
   "id": "29e0841ff52d2bff",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Using Softmax",
   "id": "4e59d5802390d14d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T22:24:52.786497Z",
     "start_time": "2025-12-01T22:24:52.780244Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tril = torch.tril(torch.ones(T , T)).to(device)\n",
    "print(tril)"
   ],
   "id": "ea56ea6711623226",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.]], device='cuda:0')\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T22:24:52.798501Z",
     "start_time": "2025-12-01T22:24:52.792502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wei = torch.zeros(T , T , device=device)\n",
    "#upper triangular part to -inf else 0 , restricting attention to previous tokens only.\n",
    "wei = wei.masked_fill(tril == 0 , float('-inf'))\n",
    "wei = F.softmax(wei , dim=1) #softmax along the rows\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow2 , xbow3) #matrices are equivalent\n",
    "\n"
   ],
   "id": "fa7145b43526f8e5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now implementing self attention mechanism.",
   "id": "2194f96b721e2e06"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T22:24:52.833561Z",
     "start_time": "2025-12-01T22:24:52.805702Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(1337)\n",
    "B , T, C = 4 , 8 , 32 #batch size , time , channels .\n",
    "x = torch.randn(B, T, C).to(device)\n",
    "\n",
    "#single head self attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C , head_size , bias=False).to(device)\n",
    "query = nn.Linear(C , head_size , bias=False).to(device)\n",
    "value = nn.Linear(C , head_size , bias=False).to(device)\n",
    "k = key(x) # BXTXhead_size\n",
    "q = query(x) # BXTXhead_size\n",
    "wei = q @ k.transpose(-2 , -1) * (head_size ** -0.5) #scaling factor , for softmax stability .\n",
    "tril = torch.tril(torch.ones(T , T)).to(device)\n",
    "#wei = torch.zeros(T , T , device=device)\n",
    "wei = wei.masked_fill(tril == 0 , float('-inf'))\n",
    "wei = F.softmax(wei , dim=-1) #softmax along the rows\n",
    "v= value(x) # BXTXhead_size\n",
    "out = wei @ v\n",
    "wei[0]"
   ],
   "id": "8286a72f5c679e96",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3966, 0.6034, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3069, 0.2892, 0.4039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3233, 0.2175, 0.2443, 0.2149, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1479, 0.2034, 0.1663, 0.1455, 0.3369, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1259, 0.2490, 0.1324, 0.1062, 0.3141, 0.0724, 0.0000, 0.0000],\n",
       "        [0.1598, 0.1990, 0.1140, 0.1125, 0.1418, 0.1669, 0.1061, 0.0000],\n",
       "        [0.0845, 0.1197, 0.1078, 0.1537, 0.1086, 0.1146, 0.1558, 0.1553]],\n",
       "       device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Bigram model  , with a single attention head .",
   "id": "f88f28926dcfbe9d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T22:24:54.778743Z",
     "start_time": "2025-12-01T22:24:52.843953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "n_embd = 32\n",
    "vocab_size = len(chars)\n",
    "\n",
    "#implelementing single head self attention as a nn.Module\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd , head_size , bias=False) #key , what to look for\n",
    "        self.query = nn.Linear(n_embd , head_size , bias=False) #query , what to match with\n",
    "        self.value = nn.Linear(n_embd , head_size , bias=False) #value , what to return\n",
    "        self.register_buffer('tril' , torch.tril(torch.ones(block_size , block_size))) #lower triangular matrix for masking\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        B , T, C = x.shape\n",
    "        k = self.key(x)     # BXTXhead_size\n",
    "        q = self.query(x)   # BXTXhead_size\n",
    "        wei = q @ k.transpose(-2 , -1) * (k.shape[-1] ** -0.5) #scaling factor , for softmax stability .\n",
    "        wei = wei.masked_fill(self.tril[:T , :T] == 0 , float('-inf'))\n",
    "        wei = F.softmax(wei , dim=-1) #softmax along the rows\n",
    "        v= self.value(x)   # BXTXhead_size\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        #each row represents a token id , and each mat[i,j] is the score for jth token in the vocab given ith token.\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) #for each token we get a vector its repr'.\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.sa_head = Head(n_embd) #self attention head\n",
    "        self.position_embedding_table = nn.Embedding(block_size , n_embd) #repr' of the position of the token in the sequence.\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B,T = idx.shape\n",
    "        #idx is the BXT tensor of input token indices .\n",
    "        token_emb = self.token_embedding_table(idx) #BXTXC.\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T , device=device)) #TXC  .\n",
    "        x = token_emb + pos_emb #BXTXC , adding token embedding and position embedding.\n",
    "        x = self.sa_head(x) #applying self attention , making the x tokens enriched with context from previous tokens.\n",
    "        logits = self.lm_head(x) #BXTXvocab_size\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B , T, C = logits.shape\n",
    "            logits = logits.view(B*T , C) # (BXT)XC. reshaping for loss computation 3d to 2d.\n",
    "            targets = targets.view(B*T) # (BXT) . reshaping for loss computation 2d to 1d.\n",
    "            #how well we are predicting the targets\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            #for each entry we will return the logits for the next token.\n",
    "        return logits , loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        #idx is a BXT tensor of input token\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:] #conditioning only on the last block_size tokens. for now 2.\n",
    "            logits , loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :] #BXC  , for each batch we take the predictions for the last token.\n",
    "            probs = F.softmax(logits, dim=-1) #BXC , converting to probabilities\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) #BX1 , the next predicted token for each batch.\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # BX(T+1) , appending the predicted token to the input sequence.\n",
    "        return idx\n",
    "\n",
    "\n",
    "m = BigramLanguageModel().to(device)\n",
    "out , loss = m(xb , yb)\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=0.001)\n",
    "#training the model , as before.\n",
    "batch_size = 32 #how many independent sequences will we process in parallel\n",
    "for steps in range(1000):\n",
    "    #get a batch of data\n",
    "    xb , yb = get_batch('train')\n",
    "    #evaluate the loss\n",
    "    logits , loss = m(xb, yb)\n",
    "    #backpropagation\n",
    "    optimizer.zero_grad(set_to_none=True) #set gradients to zero , to avoid accumulation from previous step.\n",
    "    loss.backward() # compute gradients , see how much each weight contributed to the loss.\n",
    "    optimizer.step()\n",
    "    if steps % 10 == 0:\n",
    "        print(steps, loss.item())\n"
   ],
   "id": "551d1c8e3d022349",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.333260536193848\n",
      "10 4.128382682800293\n",
      "20 3.872257947921753\n",
      "30 3.730135440826416\n",
      "40 3.5248467922210693\n",
      "50 3.2394115924835205\n",
      "60 3.3388290405273438\n",
      "70 3.3383145332336426\n",
      "80 3.2185425758361816\n",
      "90 3.2796216011047363\n",
      "100 3.1258187294006348\n",
      "110 3.121286392211914\n",
      "120 3.1905524730682373\n",
      "130 3.123028516769409\n",
      "140 2.93449068069458\n",
      "150 3.1797943115234375\n",
      "160 3.1929144859313965\n",
      "170 3.1662614345550537\n",
      "180 3.2666451930999756\n",
      "190 3.0200681686401367\n",
      "200 3.0554254055023193\n",
      "210 3.3084311485290527\n",
      "220 3.0297930240631104\n",
      "230 3.004370927810669\n",
      "240 3.0135908126831055\n",
      "250 3.049196243286133\n",
      "260 3.0346803665161133\n",
      "270 3.071700096130371\n",
      "280 2.960993528366089\n",
      "290 3.0096285343170166\n",
      "300 2.8178625106811523\n",
      "310 2.746882438659668\n",
      "320 2.7231075763702393\n",
      "330 2.8228759765625\n",
      "340 2.8194055557250977\n",
      "350 2.856890916824341\n",
      "360 2.925802707672119\n",
      "370 2.792895793914795\n",
      "380 2.92997407913208\n",
      "390 2.8448657989501953\n",
      "400 2.729879856109619\n",
      "410 2.8665823936462402\n",
      "420 2.6970646381378174\n",
      "430 2.725980043411255\n",
      "440 2.68788743019104\n",
      "450 2.6756720542907715\n",
      "460 2.7189881801605225\n",
      "470 2.8700358867645264\n",
      "480 2.7541797161102295\n",
      "490 2.6342835426330566\n",
      "500 2.859753370285034\n",
      "510 2.7521564960479736\n",
      "520 2.677006483078003\n",
      "530 2.556718349456787\n",
      "540 2.686406135559082\n",
      "550 2.483652114868164\n",
      "560 2.767169952392578\n",
      "570 2.6103591918945312\n",
      "580 2.763068675994873\n",
      "590 2.6334590911865234\n",
      "600 2.596480131149292\n",
      "610 2.694481372833252\n",
      "620 2.5336897373199463\n",
      "630 2.6579010486602783\n",
      "640 2.5764079093933105\n",
      "650 2.5688016414642334\n",
      "660 2.7050559520721436\n",
      "670 2.478294849395752\n",
      "680 2.5274658203125\n",
      "690 2.5104427337646484\n",
      "700 2.6585447788238525\n",
      "710 2.53560471534729\n",
      "720 2.7822425365448\n",
      "730 2.4898018836975098\n",
      "740 2.567286968231201\n",
      "750 2.556427240371704\n",
      "760 2.675577163696289\n",
      "770 2.5348353385925293\n",
      "780 2.7225730419158936\n",
      "790 2.730635166168213\n",
      "800 2.466625452041626\n",
      "810 2.5889511108398438\n",
      "820 2.5857770442962646\n",
      "830 2.510159969329834\n",
      "840 2.4394619464874268\n",
      "850 2.6781976222991943\n",
      "860 2.5293591022491455\n",
      "870 2.5164365768432617\n",
      "880 2.526000499725342\n",
      "890 2.5207066535949707\n",
      "900 2.5826165676116943\n",
      "910 2.6087119579315186\n",
      "920 2.509773015975952\n",
      "930 2.512909412384033\n",
      "940 2.572143316268921\n",
      "950 2.6495003700256348\n",
      "960 2.5423946380615234\n",
      "970 2.536801338195801\n",
      "980 2.5544610023498535\n",
      "990 2.4022607803344727\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T22:24:55.066001Z",
     "start_time": "2025-12-01T22:24:54.876436Z"
    }
   },
   "cell_type": "code",
   "source": "print(decode(m.generate(idx, max_new_tokens=300)[0].tolist()))",
   "id": "91d0c9be56810e1b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Whent ikitridcowi,\n",
      "T is b, bte\n",
      "\n",
      "Hiset bube ule.\n",
      "S:\n",
      "O-ans mealatauss ar bthif uw he, vete redthas ate awice my.\n",
      "\n",
      "HDEEarut orour\n",
      "Yowthertof isth bet mil ndilincaes iree sengcin lat HFridrov te, anen m pnganr.\n",
      "Trans!\n",
      "el lind me ut liser onchiry wture aiss hewty.\n",
      "Hllinte korfopetelaves\n",
      "Mk:\n",
      "Ill, dl tthak\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now multiple heads of self attention , as well as MLP (copy paste from above and modify).",
   "id": "eaf7e2f12bb2ba9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T22:24:55.344845Z",
     "start_time": "2025-12-01T22:24:55.337780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "n_embd = 32\n",
    "vocab_size = len(chars)\n",
    "\n",
    "#implelementing single head self attention as a nn.Module\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd , head_size , bias=False) #key , what to look for\n",
    "        self.query = nn.Linear(n_embd , head_size , bias=False) #query , what to match with\n",
    "        self.value = nn.Linear(n_embd , head_size , bias=False) #value , what to return\n",
    "        self.register_buffer('tril' , torch.tril(torch.ones(block_size , block_size))) #lower triangular matrix for masking\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        B , T, C = x.shape\n",
    "        k = self.key(x)     # BXTXhead_size\n",
    "        q = self.query(x)   # BXTXhead_size\n",
    "        wei = q @ k.transpose(-2 , -1) * (k.shape[-1] ** -0.5) #scaling factor , for softmax stability .\n",
    "        wei = wei.masked_fill(self.tril[:T , :T] == 0 , float('-inf'))\n",
    "        wei = F.softmax(wei , dim=-1) #softmax along the rows\n",
    "        v= self.value(x)   # BXTXhead_size\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self , num_heads , head_size):\n",
    "        super().__init__()\n",
    "        #create a list of multiple heads.\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self , x):\n",
    "        #concatenate the output of each head along the channel dimension , basiclly stacking them.\n",
    "        out = torch.cat([h(x) for h in self.heads] , dim=-1)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd ,n_embd), nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        #each row represents a token id , and each mat[i,j] is the score for jth token in the vocab given ith token.\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) #for each token we get a vector its repr'.\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.sa_heads = MultiHeadAttention(num_heads=4 , head_size=n_embd//4) #self attention head , num_heads * head_size = n_embd\n",
    "        self.ffwd = FeedForward(n_embd) #feed forward layer\n",
    "        self.position_embedding_table = nn.Embedding(block_size , n_embd) #repr' of the position of the token in the sequence.\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B,T = idx.shape\n",
    "        #idx is the BXT tensor of input token indices .\n",
    "        token_emb = self.token_embedding_table(idx) #BXTXC.\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T , device=device)) #TXC  .\n",
    "        x = token_emb + pos_emb #BXTXC , adding token embedding and position embedding.\n",
    "        x = self.sa_heads(x) #applying self attention , making the x tokens enriched with context from previous tokens.\n",
    "        x = self.ffwd(x) #applying feed forward layer.\n",
    "        logits = self.lm_head(x) #BXTXvocab_size\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B , T, C = logits.shape\n",
    "            logits = logits.view(B*T , C) # (BXT)XC. reshaping for loss computation 3d to 2d.\n",
    "            targets = targets.view(B*T) # (BXT) . reshaping for loss computation 2d to 1d.\n",
    "            #how well we are predicting the targets\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            #for each entry we will return the logits for the next token.\n",
    "        return logits , loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        #idx is a BXT tensor of input token\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:] #conditioning only on the last block_size tokens. for now 2.\n",
    "            logits , loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :] #BXC  , for each batch we take the predictions for the last token.\n",
    "            probs = F.softmax(logits, dim=-1) #BXC , converting to probabilities\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) #BX1 , the next predicted token for each batch.\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # BX(T+1) , appending the predicted token to the input sequence.\n",
    "        return idx\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "1ca3c99994791e76",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T22:24:55.422850Z",
     "start_time": "2025-12-01T22:24:55.419850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "m = BigramLanguageModel().to(device)\n",
    "out , loss = m(xb , yb)\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=0.001)\n",
    "#training the model , as before.\n",
    "batch_size = 32 #how many independent sequences will we process in parallel\n",
    "for steps in range(5000):\n",
    "    #get a batch of data\n",
    "    xb , yb = get_batch('train')\n",
    "    #evaluate the loss\n",
    "    logits , loss = m(xb, yb)\n",
    "    _ , val_loss = m(xb, yb)\n",
    "    #backpropagation\n",
    "    optimizer.zero_grad(set_to_none=True) #set gradients to zero , to avoid accumulation from previous step.\n",
    "    loss.backward() # compute gradients , see how much each weight contributed to the loss.\n",
    "    optimizer.step()\n",
    "    if steps % 10 == 0:\n",
    "        print(steps, loss.item() , val_loss.item())"
   ],
   "id": "1ac9ff7ea4beeaf2",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T22:24:55.611417Z",
     "start_time": "2025-12-01T22:24:55.446987Z"
    }
   },
   "cell_type": "code",
   "source": " print(decode(m.generate(idx, max_new_tokens=300)[0].tolist()))",
   "id": "fd0eea4b0a92a9aa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Whent ikitridcowi,\n",
      "T is b, bte\n",
      "\n",
      "Hiset bube ule.\n",
      "S:\n",
      "O-ans mealatauss ar bthif uw he, vete redthas ate awice my.\n",
      "\n",
      "HDEEarut orour\n",
      "Yowthertof isth bet mil ndilincaes iree sengcin lat HFridrov te, anen m pnganr.\n",
      "Trans!\n",
      "el lind me ut liser onchiry wture aiss hewty.\n",
      "Hllinte korfopetelaves\n",
      "Mk:\n",
      "Ill, dl tthak\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Scaled model with layer norm and residual connections.",
   "id": "11e93917e3dfe81d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T22:24:55.626422Z",
     "start_time": "2025-12-01T22:24:55.617422Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1337)\n",
    "#defiining hyperparameters\n",
    "batch_size = 32\n",
    "block_size = 128\n",
    "learning_rate = 3e-4\n",
    "n_layer = 6\n",
    "n_heads = 6\n",
    "n_embd = 384\n",
    "dropout = 0.2\n",
    "vocab_size = len(chars)\n",
    "\n",
    "#implelementing single head self attention as a nn.Module\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd , head_size , bias=False) #key , what to look for\n",
    "        self.query = nn.Linear(n_embd , head_size , bias=False) #query , what to match with\n",
    "        self.value = nn.Linear(n_embd , head_size , bias=False) #value , what to return\n",
    "        self.register_buffer('tril' , torch.tril(torch.ones(block_size , block_size))) #lower triangular matrix for masking\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        B , T, C = x.shape\n",
    "        k = self.key(x)     # BXTXhead_size\n",
    "        q = self.query(x)   # BXTXhead_size\n",
    "        wei = q @ k.transpose(-2 , -1) * (k.shape[-1] ** -0.5) #scaling factor , for softmax stability .\n",
    "        wei = wei.masked_fill(self.tril[:T , :T] == 0 , float('-inf'))\n",
    "        wei = F.softmax(wei , dim=-1) #softmax along the rows\n",
    "        wei = self.dropout(wei)\n",
    "        v= self.value(x)   # BXTXhead_size\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self , num_heads , head_size):\n",
    "        super().__init__()\n",
    "        #create a list of multiple heads.\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(num_heads*head_size, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self , x):\n",
    "        #concatenate the output of each head along the channel dimension , basiclly stacking them.\n",
    "        out = torch.cat([h(x) for h in self.heads] , dim=-1)\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd ,4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4* n_embd ,n_embd),\n",
    "            nn.Dropout(dropout) #adding dropout for regularization\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd , num_heads):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // num_heads\n",
    "        self.sa = MultiHeadAttention(num_heads , head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x)) #residual connection , normalize before transforming.\n",
    "        x = x + self.ffwd(self.ln2(x)) #residual connection , normalize before transforming.\n",
    "        return x\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        #each row represents a token id , and each mat[i,j] is the score for jth token in the vocab given ith token.\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) #for each token we get a vector its repr'.\n",
    "        self.position_embedding_table = nn.Embedding(block_size , n_embd) #repr' of the position of the token in the sequence.\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd , num_heads=n_heads) for _ in range(n_layer)]) #stacking multiple blocks\n",
    "        self.ln_f = nn.LayerNorm(n_embd) #final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B,T = idx.shape\n",
    "        #idx is the BXT tensor of input token indices .\n",
    "        token_emb = self.token_embedding_table(idx) #BXTXC.\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T , device=device)) #TXC  .\n",
    "        x = token_emb + pos_emb #BXTXC , adding token embedding and position embedding.\n",
    "        x = self.blocks(x) #applying self attention , making the x tokens enriched with context from previous tokens.\n",
    "        x = self.ln_f(x) #applying feed forward layer.\n",
    "        logits = self.lm_head(x) #BXTXvocab_size\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B , T, C = logits.shape\n",
    "            logits = logits.view(B*T , C) # (BXT)XC. reshaping for loss computation 3d to 2d.\n",
    "            targets = targets.view(B*T) # (BXT) . reshaping for loss computation 2d to 1d.\n",
    "            #how well we are predicting the targets\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            #for each entry we will return the logits for the next token.\n",
    "        return logits , loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        #idx is a BXT tensor of input token\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:] #conditioning only on the last block_size tokens. for now 2.\n",
    "            logits , loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :] #BXC  , for each batch we take the predictions for the last token.\n",
    "            probs = F.softmax(logits, dim=-1) #BXC , converting to probabilities\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) #BX1 , the next predicted token for each batch.\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # BX(T+1) , appending the predicted token to the input sequence.\n",
    "        return idx\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "b2c6930e13b04cca",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T22:24:55.634486Z",
     "start_time": "2025-12-01T22:24:55.631487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "m = BigramLanguageModel().to(device)\n",
    "out , loss = m(xb , yb)\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=0.001)\n",
    "#training the model , as before.\n",
    "batch_size = 32 #how many independent sequences will we process in parallel\n",
    "for steps in range(5000):\n",
    "    #get a batch of data\n",
    "    xb , yb = get_batch('train')\n",
    "    xb_val , yb_val = get_batch('val')\n",
    "    #evaluate the loss\n",
    "    logits , loss = m(xb, yb)\n",
    "    _ , val_loss = m(xb_val, yb_val)\n",
    "    #backpropagation\n",
    "    optimizer.zero_grad(set_to_none=True) #set gradients to zero , to avoid accumulation from previous step.\n",
    "    loss.backward() # compute gradients , see how much each weight contributed to the loss.\n",
    "    optimizer.step()\n",
    "    if steps % 10 == 0:\n",
    "        print(steps, loss.item() , val_loss.item())"
   ],
   "id": "bcb1d080e706bec8",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T22:24:55.649953Z",
     "start_time": "2025-12-01T22:24:55.642995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#saving the model state\n",
    "torch.save(m.state_dict() , 'bigram_model.pth')"
   ],
   "id": "7a04563971343b30",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T22:24:56.338337Z",
     "start_time": "2025-12-01T22:24:55.654958Z"
    }
   },
   "cell_type": "code",
   "source": "print(decode(m.generate(idx, max_new_tokens=1000)[0].tolist()))",
   "id": "2a239e9608f722d9",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (8) must match the size of tensor b (9) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[49]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28mprint\u001B[39m(decode(\u001B[43mm\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43midx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1000\u001B[39;49m\u001B[43m)\u001B[49m[\u001B[32m0\u001B[39m].tolist()))\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[41]\u001B[39m\u001B[32m, line 64\u001B[39m, in \u001B[36mBigramLanguageModel.generate\u001B[39m\u001B[34m(self, idx, max_new_tokens)\u001B[39m\n\u001B[32m     62\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(max_new_tokens):\n\u001B[32m     63\u001B[39m     idx_cond = idx[:, -block_size:] \u001B[38;5;66;03m#conditioning only on the last block_size tokens. for now 2.\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m64\u001B[39m     logits , loss = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43midx_cond\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     65\u001B[39m     logits = logits[:, -\u001B[32m1\u001B[39m, :] \u001B[38;5;66;03m#BXC  , for each batch we take the predictions for the last token.\u001B[39;00m\n\u001B[32m     66\u001B[39m     probs = F.softmax(logits, dim=-\u001B[32m1\u001B[39m) \u001B[38;5;66;03m#BXC , converting to probabilities\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LLMFromSratch\\.venv_gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LLMFromSratch\\.venv_gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[41]\u001B[39m\u001B[32m, line 47\u001B[39m, in \u001B[36mBigramLanguageModel.forward\u001B[39m\u001B[34m(self, idx, targets)\u001B[39m\n\u001B[32m     45\u001B[39m pos_emb = \u001B[38;5;28mself\u001B[39m.position_embedding_table(torch.arange(T , device=device)) \u001B[38;5;66;03m#TXC  .\u001B[39;00m\n\u001B[32m     46\u001B[39m x = token_emb + pos_emb \u001B[38;5;66;03m#BXTXC , adding token embedding and position embedding.\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m47\u001B[39m x = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msa_head\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m#applying self attention , making the x tokens enriched with context from previous tokens.\u001B[39;00m\n\u001B[32m     48\u001B[39m logits = \u001B[38;5;28mself\u001B[39m.lm_head(x) \u001B[38;5;66;03m#BXTXvocab_size\u001B[39;00m\n\u001B[32m     49\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m targets \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LLMFromSratch\\.venv_gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LLMFromSratch\\.venv_gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[41]\u001B[39m\u001B[32m, line 23\u001B[39m, in \u001B[36mHead.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m     21\u001B[39m q = \u001B[38;5;28mself\u001B[39m.query(x)   \u001B[38;5;66;03m# BXTXhead_size\u001B[39;00m\n\u001B[32m     22\u001B[39m wei = q @ k.transpose(-\u001B[32m2\u001B[39m , -\u001B[32m1\u001B[39m) * (k.shape[-\u001B[32m1\u001B[39m] ** -\u001B[32m0.5\u001B[39m) \u001B[38;5;66;03m#scaling factor , for softmax stability .\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m23\u001B[39m wei = \u001B[43mwei\u001B[49m\u001B[43m.\u001B[49m\u001B[43mmasked_fill\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtril\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43mT\u001B[49m\u001B[43m \u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43mT\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[43m==\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mfloat\u001B[39;49m\u001B[43m(\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43m-inf\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     24\u001B[39m wei = F.softmax(wei , dim=-\u001B[32m1\u001B[39m) \u001B[38;5;66;03m#softmax along the rows\u001B[39;00m\n\u001B[32m     25\u001B[39m v= \u001B[38;5;28mself\u001B[39m.value(x)   \u001B[38;5;66;03m# BXTXhead_size\u001B[39;00m\n",
      "\u001B[31mRuntimeError\u001B[39m: The size of tensor a (8) must match the size of tensor b (9) at non-singleton dimension 2"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T22:25:31.558670Z",
     "start_time": "2025-12-01T22:25:31.481281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#write the generated text to a file\n",
    "generated_text = decode(m.generate(idx, max_new_tokens=1000)[0].tolist())\n",
    "with open('generated_text.txt', 'w' , encoding='utf-8') as file:\n",
    "    file.write(generated_text)\n"
   ],
   "id": "9d6f251eefa6fa06",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[51]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m#write the generated text to a file\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m generated_text = decode(\u001B[43mm\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43midx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1000\u001B[39;49m\u001B[43m)\u001B[49m[\u001B[32m0\u001B[39m].tolist())\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[33m'\u001B[39m\u001B[33mgenerated_text.txt\u001B[39m\u001B[33m'\u001B[39m, \u001B[33m'\u001B[39m\u001B[33mw\u001B[39m\u001B[33m'\u001B[39m , encoding=\u001B[33m'\u001B[39m\u001B[33mutf-8\u001B[39m\u001B[33m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m file:\n\u001B[32m      4\u001B[39m     file.write(generated_text)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[41]\u001B[39m\u001B[32m, line 64\u001B[39m, in \u001B[36mBigramLanguageModel.generate\u001B[39m\u001B[34m(self, idx, max_new_tokens)\u001B[39m\n\u001B[32m     62\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(max_new_tokens):\n\u001B[32m     63\u001B[39m     idx_cond = idx[:, -block_size:] \u001B[38;5;66;03m#conditioning only on the last block_size tokens. for now 2.\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m64\u001B[39m     logits , loss = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43midx_cond\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     65\u001B[39m     logits = logits[:, -\u001B[32m1\u001B[39m, :] \u001B[38;5;66;03m#BXC  , for each batch we take the predictions for the last token.\u001B[39;00m\n\u001B[32m     66\u001B[39m     probs = F.softmax(logits, dim=-\u001B[32m1\u001B[39m) \u001B[38;5;66;03m#BXC , converting to probabilities\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LLMFromSratch\\.venv_gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LLMFromSratch\\.venv_gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[41]\u001B[39m\u001B[32m, line 44\u001B[39m, in \u001B[36mBigramLanguageModel.forward\u001B[39m\u001B[34m(self, idx, targets)\u001B[39m\n\u001B[32m     42\u001B[39m B,T = idx.shape\n\u001B[32m     43\u001B[39m \u001B[38;5;66;03m#idx is the BXT tensor of input token indices .\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m44\u001B[39m token_emb = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtoken_embedding_table\u001B[49m\u001B[43m(\u001B[49m\u001B[43midx\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m#BXTXC.\u001B[39;00m\n\u001B[32m     45\u001B[39m pos_emb = \u001B[38;5;28mself\u001B[39m.position_embedding_table(torch.arange(T , device=device)) \u001B[38;5;66;03m#TXC  .\u001B[39;00m\n\u001B[32m     46\u001B[39m x = token_emb + pos_emb \u001B[38;5;66;03m#BXTXC , adding token embedding and position embedding.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LLMFromSratch\\.venv_gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LLMFromSratch\\.venv_gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LLMFromSratch\\.venv_gpu\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:190\u001B[39m, in \u001B[36mEmbedding.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    189\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m190\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    191\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    192\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    193\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    194\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmax_norm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    195\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mnorm_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    196\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    197\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msparse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    198\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LLMFromSratch\\.venv_gpu\\Lib\\site-packages\\torch\\nn\\functional.py:2551\u001B[39m, in \u001B[36membedding\u001B[39m\u001B[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001B[39m\n\u001B[32m   2545\u001B[39m     \u001B[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001B[39;00m\n\u001B[32m   2546\u001B[39m     \u001B[38;5;66;03m# XXX: equivalent to\u001B[39;00m\n\u001B[32m   2547\u001B[39m     \u001B[38;5;66;03m# with torch.no_grad():\u001B[39;00m\n\u001B[32m   2548\u001B[39m     \u001B[38;5;66;03m#   torch.embedding_renorm_\u001B[39;00m\n\u001B[32m   2549\u001B[39m     \u001B[38;5;66;03m# remove once script supports set_grad_enabled\u001B[39;00m\n\u001B[32m   2550\u001B[39m     _no_grad_embedding_renorm_(weight, \u001B[38;5;28minput\u001B[39m, max_norm, norm_type)\n\u001B[32m-> \u001B[39m\u001B[32m2551\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mRuntimeError\u001B[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T22:25:34.986249Z",
     "start_time": "2025-12-01T22:25:34.901186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "model = BigramLanguageModel().to(device)\n",
    "model.load_state_dict(torch.load('bigram_model.pth'))\n",
    "m = model\n",
    "for i in range(3):\n",
    "    print(f\"iteration number {i+1}:\")\n",
    "    print(decode(m.generate(idx, max_new_tokens=300)[0].tolist()))\n",
    "    print(\"\\n\\n\")"
   ],
   "id": "52e2bf81a57793bb",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[52]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m model = \u001B[43mBigramLanguageModel\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      2\u001B[39m model.load_state_dict(torch.load(\u001B[33m'\u001B[39m\u001B[33mbigram_model.pth\u001B[39m\u001B[33m'\u001B[39m))\n\u001B[32m      3\u001B[39m m = model\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LLMFromSratch\\.venv_gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1355\u001B[39m, in \u001B[36mModule.to\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1352\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1353\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1355\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LLMFromSratch\\.venv_gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:915\u001B[39m, in \u001B[36mModule._apply\u001B[39m\u001B[34m(self, fn, recurse)\u001B[39m\n\u001B[32m    913\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[32m    914\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.children():\n\u001B[32m--> \u001B[39m\u001B[32m915\u001B[39m         \u001B[43mmodule\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    917\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[32m    918\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[32m    919\u001B[39m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[32m    920\u001B[39m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    925\u001B[39m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[32m    926\u001B[39m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LLMFromSratch\\.venv_gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:942\u001B[39m, in \u001B[36mModule._apply\u001B[39m\u001B[34m(self, fn, recurse)\u001B[39m\n\u001B[32m    938\u001B[39m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[32m    939\u001B[39m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[32m    940\u001B[39m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[32m    941\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.no_grad():\n\u001B[32m--> \u001B[39m\u001B[32m942\u001B[39m     param_applied = \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    943\u001B[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001B[32m    945\u001B[39m \u001B[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\LLMFromSratch\\.venv_gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1341\u001B[39m, in \u001B[36mModule.to.<locals>.convert\u001B[39m\u001B[34m(t)\u001B[39m\n\u001B[32m   1334\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m convert_to_format \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m t.dim() \u001B[38;5;129;01min\u001B[39;00m (\u001B[32m4\u001B[39m, \u001B[32m5\u001B[39m):\n\u001B[32m   1335\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m t.to(\n\u001B[32m   1336\u001B[39m             device,\n\u001B[32m   1337\u001B[39m             dtype \u001B[38;5;28;01mif\u001B[39;00m t.is_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t.is_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1338\u001B[39m             non_blocking,\n\u001B[32m   1339\u001B[39m             memory_format=convert_to_format,\n\u001B[32m   1340\u001B[39m         )\n\u001B[32m-> \u001B[39m\u001B[32m1341\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mt\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1342\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1343\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m.\u001B[49m\u001B[43mis_floating_point\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m.\u001B[49m\u001B[43mis_complex\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m   1344\u001B[39m \u001B[43m        \u001B[49m\u001B[43mnon_blocking\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1345\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1346\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m   1347\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(e) == \u001B[33m\"\u001B[39m\u001B[33mCannot copy out of meta tensor; no data!\u001B[39m\u001B[33m\"\u001B[39m:\n",
      "\u001B[31mRuntimeError\u001B[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "execution_count": 52
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
