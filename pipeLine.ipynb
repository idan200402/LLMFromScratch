{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T10:26:26.667710Z",
     "start_time": "2025-12-05T10:26:26.662045Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as file:\n",
    "    data = file.read()"
   ],
   "id": "c5d25d0c3060060",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T10:26:26.691928Z",
     "start_time": "2025-12-05T10:26:26.686871Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"the legth is :\", len(data))",
   "id": "5ebe983eaa2a0c50",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the legth is : 1115394\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T10:26:26.736303Z",
     "start_time": "2025-12-05T10:26:26.713893Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chars = sorted(list(set(data)))\n",
    "print(\"the unique chars are :\", ''.join(chars))\n",
    "print(\"the number of vocabulary is :\", len(chars))"
   ],
   "id": "5db3611893a58c0a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the unique chars are : \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "the number of vocabulary is : 65\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T10:26:26.780655Z",
     "start_time": "2025-12-05T10:26:26.775316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#maps from char to int\n",
    "char_to_int = {ch: i for i, ch in enumerate(chars)}\n",
    "#maps from int to char\n",
    "int_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "#function to encode a string to a list of integers\n",
    "encode = lambda s: [char_to_int[c] for c in s]\n",
    "#function to decode a list of integers to a string\n",
    "decode = lambda l: ''.join([int_to_char[i] for i in l])\n"
   ],
   "id": "eeef2977f9e1463b",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T10:26:26.802142Z",
     "start_time": "2025-12-05T10:26:26.794360Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#example usage\n",
    "enc = encode(\"mihmoret\")\n",
    "print(enc)\n",
    "dec = decode(enc)\n",
    "print(dec)\n",
    "#example of handling character not in the vocabulary\n",
    "try:\n",
    "    enc2 = encode(\"+\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error: {e} is not in the character set.\")"
   ],
   "id": "1954fe9e088905e1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51, 47, 46, 51, 53, 56, 43, 58]\n",
      "mihmoret\n",
      "Error: '+' is not in the character set.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T10:26:31.735615Z",
     "start_time": "2025-12-05T10:26:26.816328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"using device:\", device)\n",
    "\n",
    "text = data\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long, device=device)\n",
    "print(data.shape, data.dtype, data.device)\n",
    "print(data[:100])\n"
   ],
   "id": "1fa3c60baa39aa9b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "torch.Size([1115394]) torch.int64 cuda:0\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\PycharmProjects\\LLMFromSratch\\.venv_gpu\\Lib\\site-packages\\torch\\cuda\\__init__.py:287: UserWarning: \n",
      "NVIDIA GeForce RTX 5060 Laptop GPU with CUDA capability sm_120 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_61 sm_70 sm_75 sm_80 sm_86 sm_90 compute_37.\n",
      "If you want to use the NVIDIA GeForce RTX 5060 Laptop GPU GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T10:26:32.104123Z",
     "start_time": "2025-12-05T10:26:32.098865Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#split the data into train and validation sets\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ],
   "id": "dab3c18f3bfd020",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T10:26:32.114773Z",
     "start_time": "2025-12-05T10:26:32.110925Z"
    }
   },
   "cell_type": "code",
   "source": "block_size = 8",
   "id": "a58d4de97f911da2",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T10:26:32.132143Z",
     "start_time": "2025-12-05T10:26:32.123124Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#example 0: demonstrate how to create input and target sequences\n",
    "#producing input and target sequences (labels)\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size + 1]\n",
    "#for\n",
    "for t in range(block_size):\n",
    "    context = x[:t + 1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context.tolist()} the target is {target.item()}\")"
   ],
   "id": "9d84f9a2bf4ed319",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is [18] the target is 47\n",
      "when input is [18, 47] the target is 56\n",
      "when input is [18, 47, 56] the target is 57\n",
      "when input is [18, 47, 56, 57] the target is 58\n",
      "when input is [18, 47, 56, 57, 58] the target is 1\n",
      "when input is [18, 47, 56, 57, 58, 1] the target is 15\n",
      "when input is [18, 47, 56, 57, 58, 1, 15] the target is 47\n",
      "when input is [18, 47, 56, 57, 58, 1, 15, 47] the target is 58\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T10:26:32.235675Z",
     "start_time": "2025-12-05T10:26:32.209518Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4  #how many independent sequences will we process in parallel\n",
    "block_size = 8  #what is the maximum context length for predictions\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    #generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))  # 4 random starting indices along the data\n",
    "    x = torch.stack([data[i:i + block_size] for i in ix])  # 4 sequences of length block_size , 4X8 tensor.\n",
    "    y = torch.stack(\n",
    "        [data[i + 1:i + block_size + 1] for i in ix])  # 4 sequences of length block_size , 4X8 tensor.the labels.\n",
    "    return x, y\n",
    "\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "xb.to('cuda')\n",
    "yb.to('cuda')\n",
    "print(\"inputs:\")\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print(\"outputs:\")\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "print(\"---\")\n",
    "\n",
    "#example of input and target.\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t + 1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"when input is {context.tolist()} the target is {target.item()}\")"
   ],
   "id": "5692cf83aff222d5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]], device='cuda:0')\n",
      "outputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]], device='cuda:0')\n",
      "---\n",
      "when input is [24] the target is 43\n",
      "when input is [24, 43] the target is 58\n",
      "when input is [24, 43, 58] the target is 5\n",
      "when input is [24, 43, 58, 5] the target is 57\n",
      "when input is [24, 43, 58, 5, 57] the target is 1\n",
      "when input is [24, 43, 58, 5, 57, 1] the target is 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] the target is 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target is 39\n",
      "when input is [44] the target is 53\n",
      "when input is [44, 53] the target is 56\n",
      "when input is [44, 53, 56] the target is 1\n",
      "when input is [44, 53, 56, 1] the target is 58\n",
      "when input is [44, 53, 56, 1, 58] the target is 46\n",
      "when input is [44, 53, 56, 1, 58, 46] the target is 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] the target is 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target is 1\n",
      "when input is [52] the target is 58\n",
      "when input is [52, 58] the target is 1\n",
      "when input is [52, 58, 1] the target is 58\n",
      "when input is [52, 58, 1, 58] the target is 46\n",
      "when input is [52, 58, 1, 58, 46] the target is 39\n",
      "when input is [52, 58, 1, 58, 46, 39] the target is 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] the target is 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target is 46\n",
      "when input is [25] the target is 17\n",
      "when input is [25, 17] the target is 27\n",
      "when input is [25, 17, 27] the target is 10\n",
      "when input is [25, 17, 27, 10] the target is 0\n",
      "when input is [25, 17, 27, 10, 0] the target is 21\n",
      "when input is [25, 17, 27, 10, 0, 21] the target is 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] the target is 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target is 39\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Creating the bigram language model And training it.",
   "id": "ee8f310d789beced"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T10:26:34.007541Z",
     "start_time": "2025-12-05T10:26:32.253881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#bigram model\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        #each row represents a token id , and each mat[i,j] is the score for jth token in the vocab given ith token.\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        #idx is the BXT tensor of input token indices .\n",
    "        logits = self.token_embedding_table(idx)  #BXTXC.\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)  # (BXT)XC. reshaping for loss computation 3d to 2d.\n",
    "            targets = targets.view(B * T)  # (BXT) . reshaping for loss computation 2d to 1d.\n",
    "            #how well we are predicting the targets\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            #for each entry we will return the logits for the next token.\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        #idx is a BXT tensor of input token\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)\n",
    "            logits = logits[:, -1, :]  #BXC  , for each batch we take the predictions for the last token.\n",
    "            probs = F.softmax(logits, dim=-1)  #BXC , converting to probabilities\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  #BX1 , the next predicted token for each batch.\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # BX(T+1) , appending the predicted token to the input sequence.\n",
    "        return idx\n",
    "\n",
    "\n",
    "m = BigramLanguageModel(vocab_size=len(chars)).to(device)\n",
    "out, loss = m(xb, yb)\n",
    "print(out)\n",
    "print(loss)\n",
    "\n",
    "# a tensor with a single zero , B=1 , T=1 , representing the start token.\n",
    "idx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "#generating 100 new tokens\n",
    "print(decode(m.generate(idx, max_new_tokens=100)[0].tolist()))\n"
   ],
   "id": "65dca01a0850bff7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5101, -0.0948,  1.0927,  ..., -0.6126, -0.6597,  0.7624],\n",
      "        [ 0.3323, -0.0872, -0.7470,  ..., -0.6716, -0.9572, -0.9594],\n",
      "        [ 0.2475, -0.6349, -1.2909,  ...,  1.3064, -0.2256, -1.8305],\n",
      "        ...,\n",
      "        [-2.1910, -0.7574,  1.9656,  ..., -0.3580,  0.8585, -0.6161],\n",
      "        [ 0.5978, -0.0514, -0.0646,  ..., -1.4649, -2.0555,  1.8275],\n",
      "        [-0.6787,  0.8662, -1.6433,  ...,  2.3671, -0.7775, -0.2586]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "tensor(4.8786, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "\n",
      "pYCXxfRkRZd\n",
      "wc'wfNfT;OLlTEeC K\n",
      "jxqPToTb?bXAUG:C-SGJO-33SM:C?YI3a\n",
      "hs:LVXJFhXeNuwqhObxZ.tSVrddXlaSZaNe\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T10:26:35.323605Z",
     "start_time": "2025-12-05T10:26:34.040734Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#training the model , defining the optimizer.\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ],
   "id": "d8f563dacc35c592",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T10:26:36.867611Z",
     "start_time": "2025-12-05T10:26:35.356686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 32  #how many independent sequences will we process in parallel\n",
    "for steps in range(1000):\n",
    "#get a batch of data\n",
    "xb, yb = get_batch('train')\n",
    "#evaluate the loss\n",
    "logits, loss = m(xb, yb)\n",
    "#backpropagation\n",
    "optimizer.zero_grad(set_to_none=True)  #set gradients to zero , to avoid accumulation from previous step.\n",
    "loss.backward()  # compute gradients , see how much each weight contributed to the loss.\n",
    "optimizer.step()\n",
    "if steps % 10 == 0:\n",
    "    print(steps, loss.item())"
   ],
   "id": "bad5917a434639c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.648484230041504\n",
      "10 4.74024772644043\n",
      "20 4.635769367218018\n",
      "30 4.7977776527404785\n",
      "40 4.6954665184021\n",
      "50 4.581179618835449\n",
      "60 4.682138919830322\n",
      "70 4.519830703735352\n",
      "80 4.635293006896973\n",
      "90 4.661656856536865\n",
      "100 4.642975807189941\n",
      "110 4.68632698059082\n",
      "120 4.497181415557861\n",
      "130 4.556583881378174\n",
      "140 4.6776275634765625\n",
      "150 4.449394226074219\n",
      "160 4.50913143157959\n",
      "170 4.5243000984191895\n",
      "180 4.548206806182861\n",
      "190 4.4104084968566895\n",
      "200 4.47345495223999\n",
      "210 4.459536075592041\n",
      "220 4.4392805099487305\n",
      "230 4.460430145263672\n",
      "240 4.4244866371154785\n",
      "250 4.42477560043335\n",
      "260 4.36106014251709\n",
      "270 4.415848731994629\n",
      "280 4.46035099029541\n",
      "290 4.366547107696533\n",
      "300 4.2544732093811035\n",
      "310 4.222301483154297\n",
      "320 4.382016658782959\n",
      "330 4.376062870025635\n",
      "340 4.3820414543151855\n",
      "350 4.366427898406982\n",
      "360 4.201217174530029\n",
      "370 4.353005886077881\n",
      "380 4.281277656555176\n",
      "390 4.290494918823242\n",
      "400 4.29338264465332\n",
      "410 4.132618427276611\n",
      "420 4.231373310089111\n",
      "430 4.197425365447998\n",
      "440 4.257749557495117\n",
      "450 4.179753303527832\n",
      "460 4.276759147644043\n",
      "470 4.148818492889404\n",
      "480 4.204633712768555\n",
      "490 4.236260414123535\n",
      "500 4.162595748901367\n",
      "510 4.14796781539917\n",
      "520 4.220947742462158\n",
      "530 4.178046703338623\n",
      "540 4.100080490112305\n",
      "550 4.201590538024902\n",
      "560 4.042474746704102\n",
      "570 3.9905407428741455\n",
      "580 4.056334018707275\n",
      "590 4.065803527832031\n",
      "600 4.0203423500061035\n",
      "610 4.006284236907959\n",
      "620 4.049383640289307\n",
      "630 4.1103739738464355\n",
      "640 4.08767557144165\n",
      "650 3.985372304916382\n",
      "660 3.986151695251465\n",
      "670 4.013110160827637\n",
      "680 4.0221686363220215\n",
      "690 3.9954051971435547\n",
      "700 4.030514240264893\n",
      "710 4.110603332519531\n",
      "720 3.9340009689331055\n",
      "730 3.965081214904785\n",
      "740 3.85903263092041\n",
      "750 3.925828218460083\n",
      "760 3.941173553466797\n",
      "770 3.9219741821289062\n",
      "780 3.911132574081421\n",
      "790 3.8220386505126953\n",
      "800 3.8753600120544434\n",
      "810 3.831495761871338\n",
      "820 3.937608480453491\n",
      "830 4.014145374298096\n",
      "840 3.8120458126068115\n",
      "850 3.8790812492370605\n",
      "860 3.7466893196105957\n",
      "870 3.8935012817382812\n",
      "880 3.770812749862671\n",
      "890 3.7455272674560547\n",
      "900 3.810453414916992\n",
      "910 3.8390934467315674\n",
      "920 3.837909698486328\n",
      "930 3.809741258621216\n",
      "940 3.7322657108306885\n",
      "950 3.743919849395752\n",
      "960 3.765810012817383\n",
      "970 3.795292854309082\n",
      "980 3.77083683013916\n",
      "990 3.720672130584717\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T10:26:36.974675Z",
     "start_time": "2025-12-05T10:26:36.901037Z"
    }
   },
   "cell_type": "code",
   "source": [
    " #geenrating a  1 degree  tensor of predicted tokens , then decoding and printing them.\n",
    "print(decode(m.generate(idx, max_new_tokens=300)[0].tolist()))\n",
    "out, loss = m(xb, yb)\n",
    "print(out)\n",
    "print(loss)"
   ],
   "id": "675eb969be52677a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "W?w3cHPyZWk,f's$a-oizCjmuX\n",
      "YoR&$FMVofXisEvB!!BA!$W:CdYlixcaeg ireeYERnkcin;lxWiHFliqmoGSKtSV&BLqWk p.SGFo.\n",
      "SGjbo!UelIlind,pea!.\n",
      "-huD3SPyckzby:CUup;MOissX3Qwty.OJlvBPUSIkyBf&patelgCIEJMk:Chll,SPlyltSPkqmoRW-wNAXQbjxCevib3sr'T:C-&dE$HZAETENehhir$Fstp-LK3:CJ-xTrg\n",
      "\n",
      "ALkOdmnunruf?qA so;;3QQkhWTE:CEEwfep$v\n",
      "tensor([[ 0.3500,  0.8383, -2.9546,  ..., -1.4349, -0.3560, -0.1226],\n",
      "        [-0.6300,  0.2204, -0.5048,  ..., -1.4783, -1.4049, -1.2824],\n",
      "        [-0.7509, -0.7660, -0.1990,  ...,  1.3423,  1.9044, -0.2119],\n",
      "        ...,\n",
      "        [ 0.2930,  0.8083, -0.3381,  ..., -1.4258, -1.4286, -0.6515],\n",
      "        [ 0.3926,  0.8269, -0.7454,  ..., -0.7713, -0.6833, -1.7850],\n",
      "        [-0.3034, -0.9346, -0.9612,  ..., -2.3500, -1.3458,  0.9166]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "tensor(3.8054, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A toy example of how tokens refer to each other during self attention.",
   "id": "54e83475f4a28cac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T10:26:37.008386Z",
     "start_time": "2025-12-05T10:26:36.993375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2  #batch size , time steps , channels\n",
    "x = torch.randn(B, T, C).to(device)\n",
    "x.shape"
   ],
   "id": "c6462b1b4eaa1694",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The goal  , make the token in time t hold semantic information about all previous tokens from time 0 to t.\n",
    "First attempt , with nested for loops."
   ],
   "id": "8999ca6adbe3dc9b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T10:26:37.123757Z",
     "start_time": "2025-12-05T10:26:37.042259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "xbow = torch.zeros((B, T, C), device=device)  # bow is \"bag of words\"\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t + 1]  #t+1 because we want to include the current time step  . TXC\n",
    "        xbow[b, t] = torch.mean(xprev, dim=0)  #average"
   ],
   "id": "d9207b50085b153c",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T10:26:37.147081Z",
     "start_time": "2025-12-05T10:26:37.133274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"the tokens for batch 0:\", x[0])\n",
    "print(\"enriched tokens for batch 0 :\", xbow[0])"
   ],
   "id": "348819e187af7949",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the tokens for batch 0: tensor([[ 0.1808, -0.0700],\n",
      "        [-0.3596, -0.9152],\n",
      "        [ 0.6258,  0.0255],\n",
      "        [ 0.9545,  0.0643],\n",
      "        [ 0.3612,  1.1679],\n",
      "        [-1.3499, -0.5102],\n",
      "        [ 0.2360, -0.2398],\n",
      "        [-0.9211,  1.5433]], device='cuda:0')\n",
      "enriched tokens for batch 0 : tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]], device='cuda:0')\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Second attempt , using matrix multiplication.",
   "id": "505fc40dd7348efd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T10:26:37.297991Z",
     "start_time": "2025-12-05T10:26:37.165413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3)).to(device)\n",
    "a = a / torch.sum(a, dim=1, keepdim=True)  #normalize so that each row sums to 1\n",
    "b = torch.randint(0, 10, (3, 2)).float().to(device)\n",
    "c = a @ b\n",
    "print(\"a:\")\n",
    "print(a)\n",
    "print(\"b:\")\n",
    "print(b)\n",
    "print(\"c:\")\n",
    "print(c)"
   ],
   "id": "5ca2d10d9d6a5bc9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]], device='cuda:0')\n",
      "b:\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]], device='cuda:0')\n",
      "c:\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]], device='cuda:0')\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T10:26:37.486782Z",
     "start_time": "2025-12-05T10:26:37.315841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wei = torch.tril(torch.ones(T, T)).to(device)\n",
    "wei = wei / torch.sum(wei, dim=1, keepdim=True)\n",
    "xbow2 = wei @ x  # TXT X BXTXC ---> BXTXT X BXTXC --> BXTXC\n",
    "b = 1\n",
    "\n",
    "torch.allclose(xbow, xbow2, atol=1e-6, rtol=1e-5)\n"
   ],
   "id": "29e0841ff52d2bff",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Using Softmax",
   "id": "4e59d5802390d14d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T10:26:37.527866Z",
     "start_time": "2025-12-05T10:26:37.513044Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tril = torch.tril(torch.ones(T, T)).to(device)\n",
    "print(tril)"
   ],
   "id": "ea56ea6711623226",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.]], device='cuda:0')\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T10:26:37.551290Z",
     "start_time": "2025-12-05T10:26:37.540233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wei = torch.zeros(T, T, device=device)\n",
    "#upper triangular part to -inf else 0 , restricting attention to previous tokens only.\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=1)  #softmax along the rows\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow2, xbow3)  #matrices are equivalent\n",
    "\n"
   ],
   "id": "fa7145b43526f8e5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now implementing self attention mechanism.",
   "id": "2194f96b721e2e06"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T10:26:37.606919Z",
     "start_time": "2025-12-05T10:26:37.562329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32  #batch size , time , channels .\n",
    "x = torch.randn(B, T, C).to(device)\n",
    "\n",
    "#single head self attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False).to(device)\n",
    "query = nn.Linear(C, head_size, bias=False).to(device)\n",
    "value = nn.Linear(C, head_size, bias=False).to(device)\n",
    "k = key(x)  # BXTXhead_size\n",
    "q = query(x)  # BXTXhead_size\n",
    "wei = q @ k.transpose(-2, -1) * (head_size ** -0.5)  #scaling factor , for softmax stability .\n",
    "tril = torch.tril(torch.ones(T, T)).to(device)\n",
    "#wei = torch.zeros(T , T , device=device)\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)  #softmax along the rows\n",
    "v = value(x)  # BXTXhead_size\n",
    "out = wei @ v\n",
    "wei[0]"
   ],
   "id": "8286a72f5c679e96",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3966, 0.6034, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3069, 0.2892, 0.4039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3233, 0.2175, 0.2443, 0.2149, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1479, 0.2034, 0.1663, 0.1455, 0.3369, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1259, 0.2490, 0.1324, 0.1062, 0.3141, 0.0724, 0.0000, 0.0000],\n",
       "        [0.1598, 0.1990, 0.1140, 0.1125, 0.1418, 0.1669, 0.1061, 0.0000],\n",
       "        [0.0845, 0.1197, 0.1078, 0.1537, 0.1086, 0.1146, 0.1558, 0.1553]],\n",
       "       device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Bigram model  , with a single attention head .",
   "id": "f88f28926dcfbe9d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T10:26:40.145725Z",
     "start_time": "2025-12-05T10:26:37.645241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "n_embd = 32\n",
    "vocab_size = len(chars)\n",
    "\n",
    "\n",
    "#implelementing single head self attention as a nn.Module\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)  #key , what to look for\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)  #query , what to match with\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)  #value , what to return\n",
    "        self.register_buffer('tril',\n",
    "                             torch.tril(torch.ones(block_size, block_size)))  #lower triangular matrix for masking\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)  # BXTXhead_size\n",
    "        q = self.query(x)  # BXTXhead_size\n",
    "        wei = q @ k.transpose(-2, -1) * (k.shape[-1] ** -0.5)  #scaling factor , for softmax stability .\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)  #softmax along the rows\n",
    "        v = self.value(x)  # BXTXhead_size\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        #each row represents a token id , and each mat[i,j] is the score for jth token in the vocab given ith token.\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)  #for each token we get a vector its repr'.\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.sa_head = Head(n_embd)  #self attention head\n",
    "        self.position_embedding_table = nn.Embedding(block_size,\n",
    "                                                     n_embd)  #repr' of the position of the token in the sequence.\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        #idx is the BXT tensor of input token indices .\n",
    "        token_emb = self.token_embedding_table(idx)  #BXTXC.\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  #TXC  .\n",
    "        x = token_emb + pos_emb  #BXTXC , adding token embedding and position embedding.\n",
    "        x = self.sa_head(x)  #applying self attention , making the x tokens enriched with context from previous tokens.\n",
    "        logits = self.lm_head(x)  #BXTXvocab_size\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)  # (BXT)XC. reshaping for loss computation 3d to 2d.\n",
    "            targets = targets.view(B * T)  # (BXT) . reshaping for loss computation 2d to 1d.\n",
    "            #how well we are predicting the targets\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            #for each entry we will return the logits for the next token.\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        #idx is a BXT tensor of input token\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]  #conditioning only on the last block_size tokens. for now 2.\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]  #BXC  , for each batch we take the predictions for the last token.\n",
    "            probs = F.softmax(logits, dim=-1)  #BXC , converting to probabilities\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  #BX1 , the next predicted token for each batch.\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # BX(T+1) , appending the predicted token to the input sequence.\n",
    "        return idx\n",
    "\n",
    "\n",
    "m = BigramLanguageModel().to(device)\n",
    "out, loss = m(xb, yb)\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=0.001)\n",
    "#training the model , as before.\n",
    "batch_size = 32  #how many independent sequences will we process in parallel\n",
    "for steps in range(1000):\n",
    "    #get a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    #evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    #backpropagation\n",
    "    optimizer.zero_grad(set_to_none=True)  #set gradients to zero , to avoid accumulation from previous step.\n",
    "    loss.backward()  # compute gradients , see how much each weight contributed to the loss.\n",
    "    optimizer.step()\n",
    "    if steps % 10 == 0:\n",
    "        print(steps, loss.item())\n"
   ],
   "id": "551d1c8e3d022349",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.333260536193848\n",
      "10 4.128382682800293\n",
      "20 3.872257947921753\n",
      "30 3.730135440826416\n",
      "40 3.5248467922210693\n",
      "50 3.2394115924835205\n",
      "60 3.3388290405273438\n",
      "70 3.3383145332336426\n",
      "80 3.2185425758361816\n",
      "90 3.2796216011047363\n",
      "100 3.1258187294006348\n",
      "110 3.121286392211914\n",
      "120 3.1905524730682373\n",
      "130 3.123028516769409\n",
      "140 2.93449068069458\n",
      "150 3.1797943115234375\n",
      "160 3.1929144859313965\n",
      "170 3.1662614345550537\n",
      "180 3.2666451930999756\n",
      "190 3.0200681686401367\n",
      "200 3.0554254055023193\n",
      "210 3.3084311485290527\n",
      "220 3.0297930240631104\n",
      "230 3.004370927810669\n",
      "240 3.0135908126831055\n",
      "250 3.049196243286133\n",
      "260 3.0346803665161133\n",
      "270 3.071700096130371\n",
      "280 2.960993528366089\n",
      "290 3.0096285343170166\n",
      "300 2.8178625106811523\n",
      "310 2.746882438659668\n",
      "320 2.7231075763702393\n",
      "330 2.8228759765625\n",
      "340 2.8194055557250977\n",
      "350 2.856890916824341\n",
      "360 2.925802707672119\n",
      "370 2.792895793914795\n",
      "380 2.92997407913208\n",
      "390 2.8448657989501953\n",
      "400 2.729879856109619\n",
      "410 2.8665823936462402\n",
      "420 2.6970646381378174\n",
      "430 2.725980043411255\n",
      "440 2.68788743019104\n",
      "450 2.6756720542907715\n",
      "460 2.7189881801605225\n",
      "470 2.8700358867645264\n",
      "480 2.7541797161102295\n",
      "490 2.6342835426330566\n",
      "500 2.859753370285034\n",
      "510 2.7521564960479736\n",
      "520 2.677006483078003\n",
      "530 2.556718349456787\n",
      "540 2.686406135559082\n",
      "550 2.483652114868164\n",
      "560 2.767169952392578\n",
      "570 2.6103591918945312\n",
      "580 2.763068675994873\n",
      "590 2.6334590911865234\n",
      "600 2.596480131149292\n",
      "610 2.694481372833252\n",
      "620 2.5336897373199463\n",
      "630 2.6579010486602783\n",
      "640 2.5764079093933105\n",
      "650 2.5688016414642334\n",
      "660 2.7050559520721436\n",
      "670 2.478294849395752\n",
      "680 2.5274658203125\n",
      "690 2.5104427337646484\n",
      "700 2.6585447788238525\n",
      "710 2.53560471534729\n",
      "720 2.7822425365448\n",
      "730 2.4898018836975098\n",
      "740 2.567286968231201\n",
      "750 2.556427240371704\n",
      "760 2.675577163696289\n",
      "770 2.5348353385925293\n",
      "780 2.7225730419158936\n",
      "790 2.730635166168213\n",
      "800 2.466625452041626\n",
      "810 2.5889511108398438\n",
      "820 2.5857770442962646\n",
      "830 2.510159969329834\n",
      "840 2.4394619464874268\n",
      "850 2.6781976222991943\n",
      "860 2.5293591022491455\n",
      "870 2.5164365768432617\n",
      "880 2.526000499725342\n",
      "890 2.5207066535949707\n",
      "900 2.5826165676116943\n",
      "910 2.6087119579315186\n",
      "920 2.509773015975952\n",
      "930 2.512909412384033\n",
      "940 2.572143316268921\n",
      "950 2.6495003700256348\n",
      "960 2.5423946380615234\n",
      "970 2.536801338195801\n",
      "980 2.5544610023498535\n",
      "990 2.4022607803344727\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T10:26:40.602714Z",
     "start_time": "2025-12-05T10:26:40.295711Z"
    }
   },
   "cell_type": "code",
   "source": "print(decode(m.generate(idx, max_new_tokens=300)[0].tolist()))",
   "id": "91d0c9be56810e1b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Whent ikitridcowi,\n",
      "T is b, bte\n",
      "\n",
      "Hiset bube ule.\n",
      "S:\n",
      "O-ans mealatauss ar bthif uw he, vete redthas ate awice my.\n",
      "\n",
      "HDEEarut orour\n",
      "Yowthertof isth bet mil ndilincaes iree sengcin lat HFridrov te, anen m pnganr.\n",
      "Trans!\n",
      "el lind me ut liser onchiry wture aiss hewty.\n",
      "Hllinte korfopetelaves\n",
      "Mk:\n",
      "Ill, dl tthak\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now multiple heads of self attention , as well as MLP (copy paste from above and modify).",
   "id": "eaf7e2f12bb2ba9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T10:26:40.815975Z",
     "start_time": "2025-12-05T10:26:40.804494Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "n_embd = 32\n",
    "vocab_size = len(chars)\n",
    "\n",
    "\n",
    "#implelementing single head self attention as a nn.Module\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)  #key , what to look for\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)  #query , what to match with\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)  #value , what to return\n",
    "        self.register_buffer('tril',\n",
    "                             torch.tril(torch.ones(block_size, block_size)))  #lower triangular matrix for masking\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)  # BXTXhead_size\n",
    "        q = self.query(x)  # BXTXhead_size\n",
    "        wei = q @ k.transpose(-2, -1) * (k.shape[-1] ** -0.5)  #scaling factor , for softmax stability .\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)  #softmax along the rows\n",
    "        v = self.value(x)  # BXTXhead_size\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        #create a list of multiple heads.\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        #concatenate the output of each head along the channel dimension , basiclly stacking them.\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd), nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        #each row represents a token id , and each mat[i,j] is the score for jth token in the vocab given ith token.\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)  #for each token we get a vector its repr'.\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.sa_heads = MultiHeadAttention(num_heads=4,\n",
    "                                           head_size=n_embd // 4)  #self attention head , num_heads * head_size = n_embd\n",
    "        self.ffwd = FeedForward(n_embd)  #feed forward layer\n",
    "        self.position_embedding_table = nn.Embedding(block_size,\n",
    "                                                     n_embd)  #repr' of the position of the token in the sequence.\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        #idx is the BXT tensor of input token indices .\n",
    "        token_emb = self.token_embedding_table(idx)  #BXTXC.\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  #TXC  .\n",
    "        x = token_emb + pos_emb  #BXTXC , adding token embedding and position embedding.\n",
    "        x = self.sa_heads(x)  #applying self attention , making the x tokens enriched with context from previous tokens.\n",
    "        x = self.ffwd(x)  #applying feed forward layer.\n",
    "        logits = self.lm_head(x)  #BXTXvocab_size\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)  # (BXT)XC. reshaping for loss computation 3d to 2d.\n",
    "            targets = targets.view(B * T)  # (BXT) . reshaping for loss computation 2d to 1d.\n",
    "            #how well we are predicting the targets\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            #for each entry we will return the logits for the next token.\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        #idx is a BXT tensor of input token\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]  #conditioning only on the last block_size tokens. for now 2.\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]  #BXC  , for each batch we take the predictions for the last token.\n",
    "            probs = F.softmax(logits, dim=-1)  #BXC , converting to probabilities\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  #BX1 , the next predicted token for each batch.\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # BX(T+1) , appending the predicted token to the input sequence.\n",
    "        return idx\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "1ca3c99994791e76",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T10:27:43.513349Z",
     "start_time": "2025-12-05T10:26:40.878481Z"
    }
   },
   "cell_type": "code",
   "source": [
    "m = BigramLanguageModel().to(device)\n",
    "out, loss = m(xb, yb)\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=0.001)\n",
    "#training the model , as before.\n",
    "batch_size = 32  #how many independent sequences will we process in parallel\n",
    "for steps in range(5000):\n",
    "    #get a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    #evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    _, val_loss = m(xb, yb)\n",
    "    #backpropagation\n",
    "    optimizer.zero_grad(set_to_none=True)  #set gradients to zero , to avoid accumulation from previous step.\n",
    "    loss.backward()  # compute gradients , see how much each weight contributed to the loss.\n",
    "    optimizer.step()\n",
    "    if steps % 10 == 0:\n",
    "        print(steps, loss.item(), val_loss.item())"
   ],
   "id": "1ac9ff7ea4beeaf2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.192056655883789 4.192056655883789\n",
      "10 4.111555099487305 4.111555099487305\n",
      "20 3.9679033756256104 3.9679033756256104\n",
      "30 3.6774442195892334 3.6774442195892334\n",
      "40 3.5187385082244873 3.5187385082244873\n",
      "50 3.4409546852111816 3.4409546852111816\n",
      "60 3.3001418113708496 3.3001418113708496\n",
      "70 3.378511428833008 3.378511428833008\n",
      "80 3.2491354942321777 3.2491354942321777\n",
      "90 3.202897310256958 3.202897310256958\n",
      "100 3.1519384384155273 3.1519384384155273\n",
      "110 3.0915019512176514 3.0915019512176514\n",
      "120 2.9967360496520996 2.9967360496520996\n",
      "130 3.2203316688537598 3.2203316688537598\n",
      "140 2.9413657188415527 2.9413657188415527\n",
      "150 3.016359329223633 3.016359329223633\n",
      "160 2.939711809158325 2.939711809158325\n",
      "170 3.0722484588623047 3.0722484588623047\n",
      "180 2.778496742248535 2.778496742248535\n",
      "190 2.836361885070801 2.836361885070801\n",
      "200 2.9132328033447266 2.9132328033447266\n",
      "210 2.860438346862793 2.860438346862793\n",
      "220 3.074220657348633 3.074220657348633\n",
      "230 2.783539295196533 2.783539295196533\n",
      "240 2.8527297973632812 2.8527297973632812\n",
      "250 2.8826217651367188 2.8826217651367188\n",
      "260 2.7839667797088623 2.7839667797088623\n",
      "270 2.661118268966675 2.661118268966675\n",
      "280 2.8972699642181396 2.8972699642181396\n",
      "290 2.8361401557922363 2.8361401557922363\n",
      "300 2.855884075164795 2.855884075164795\n",
      "310 2.6968061923980713 2.6968061923980713\n",
      "320 2.7941808700561523 2.7941808700561523\n",
      "330 2.7714040279388428 2.7714040279388428\n",
      "340 2.8972625732421875 2.8972625732421875\n",
      "350 2.727184295654297 2.727184295654297\n",
      "360 2.7548828125 2.7548828125\n",
      "370 2.6644089221954346 2.6644089221954346\n",
      "380 2.740298271179199 2.740298271179199\n",
      "390 2.5264623165130615 2.5264623165130615\n",
      "400 2.550525188446045 2.550525188446045\n",
      "410 2.6775760650634766 2.6775760650634766\n",
      "420 2.648550510406494 2.648550510406494\n",
      "430 2.6338050365448 2.6338050365448\n",
      "440 2.603276491165161 2.603276491165161\n",
      "450 2.6991183757781982 2.6991183757781982\n",
      "460 2.6055421829223633 2.6055421829223633\n",
      "470 2.597440004348755 2.597440004348755\n",
      "480 2.437591552734375 2.437591552734375\n",
      "490 2.675011396408081 2.675011396408081\n",
      "500 2.6756906509399414 2.6756906509399414\n",
      "510 2.5581257343292236 2.5581257343292236\n",
      "520 2.6979215145111084 2.6979215145111084\n",
      "530 2.66731333732605 2.66731333732605\n",
      "540 2.4352641105651855 2.4352641105651855\n",
      "550 2.5269298553466797 2.5269298553466797\n",
      "560 2.669130563735962 2.669130563735962\n",
      "570 2.543480157852173 2.543480157852173\n",
      "580 2.608764410018921 2.608764410018921\n",
      "590 2.6630361080169678 2.6630361080169678\n",
      "600 2.4901845455169678 2.4901845455169678\n",
      "610 2.6185336112976074 2.6185336112976074\n",
      "620 2.48591947555542 2.48591947555542\n",
      "630 2.4768340587615967 2.4768340587615967\n",
      "640 2.5461924076080322 2.5461924076080322\n",
      "650 2.4240989685058594 2.4240989685058594\n",
      "660 2.378577947616577 2.378577947616577\n",
      "670 2.5612146854400635 2.5612146854400635\n",
      "680 2.477762460708618 2.477762460708618\n",
      "690 2.5827205181121826 2.5827205181121826\n",
      "700 2.4717743396759033 2.4717743396759033\n",
      "710 2.6337220668792725 2.6337220668792725\n",
      "720 2.517167091369629 2.517167091369629\n",
      "730 2.551989793777466 2.551989793777466\n",
      "740 2.524080276489258 2.524080276489258\n",
      "750 2.460315227508545 2.460315227508545\n",
      "760 2.6119983196258545 2.6119983196258545\n",
      "770 2.52850341796875 2.52850341796875\n",
      "780 2.601362705230713 2.601362705230713\n",
      "790 2.567570924758911 2.567570924758911\n",
      "800 2.4739866256713867 2.4739866256713867\n",
      "810 2.549870729446411 2.549870729446411\n",
      "820 2.4359116554260254 2.4359116554260254\n",
      "830 2.380993127822876 2.380993127822876\n",
      "840 2.4130191802978516 2.4130191802978516\n",
      "850 2.4082953929901123 2.4082953929901123\n",
      "860 2.557827949523926 2.557827949523926\n",
      "870 2.478086233139038 2.478086233139038\n",
      "880 2.475970983505249 2.475970983505249\n",
      "890 2.4329702854156494 2.4329702854156494\n",
      "900 2.5310909748077393 2.5310909748077393\n",
      "910 2.4853553771972656 2.4853553771972656\n",
      "920 2.4150776863098145 2.4150776863098145\n",
      "930 2.4560039043426514 2.4560039043426514\n",
      "940 2.4894027709960938 2.4894027709960938\n",
      "950 2.4049038887023926 2.4049038887023926\n",
      "960 2.395721197128296 2.395721197128296\n",
      "970 2.49794340133667 2.49794340133667\n",
      "980 2.282166004180908 2.282166004180908\n",
      "990 2.378106117248535 2.378106117248535\n",
      "1000 2.3361968994140625 2.3361968994140625\n",
      "1010 2.5161569118499756 2.5161569118499756\n",
      "1020 2.414323329925537 2.414323329925537\n",
      "1030 2.3272838592529297 2.3272838592529297\n",
      "1040 2.355215549468994 2.355215549468994\n",
      "1050 2.4532206058502197 2.4532206058502197\n",
      "1060 2.4247000217437744 2.4247000217437744\n",
      "1070 2.5590641498565674 2.5590641498565674\n",
      "1080 2.3197574615478516 2.3197574615478516\n",
      "1090 2.3236470222473145 2.3236470222473145\n",
      "1100 2.5174155235290527 2.5174155235290527\n",
      "1110 2.4562716484069824 2.4562716484069824\n",
      "1120 2.5395822525024414 2.5395822525024414\n",
      "1130 2.3057167530059814 2.3057167530059814\n",
      "1140 2.5173535346984863 2.5173535346984863\n",
      "1150 2.44342041015625 2.44342041015625\n",
      "1160 2.5945568084716797 2.5945568084716797\n",
      "1170 2.3813507556915283 2.3813507556915283\n",
      "1180 2.470304489135742 2.470304489135742\n",
      "1190 2.3938398361206055 2.3938398361206055\n",
      "1200 2.4543657302856445 2.4543657302856445\n",
      "1210 2.532165765762329 2.532165765762329\n",
      "1220 2.475734233856201 2.475734233856201\n",
      "1230 2.4219584465026855 2.4219584465026855\n",
      "1240 2.316929817199707 2.316929817199707\n",
      "1250 2.4776389598846436 2.4776389598846436\n",
      "1260 2.306138038635254 2.306138038635254\n",
      "1270 2.555327892303467 2.555327892303467\n",
      "1280 2.4508538246154785 2.4508538246154785\n",
      "1290 2.3873708248138428 2.3873708248138428\n",
      "1300 2.488539457321167 2.488539457321167\n",
      "1310 2.5831961631774902 2.5831961631774902\n",
      "1320 2.3805549144744873 2.3805549144744873\n",
      "1330 2.4466240406036377 2.4466240406036377\n",
      "1340 2.3810336589813232 2.3810336589813232\n",
      "1350 2.420332193374634 2.420332193374634\n",
      "1360 2.5262749195098877 2.5262749195098877\n",
      "1370 2.3143630027770996 2.3143630027770996\n",
      "1380 2.417706251144409 2.417706251144409\n",
      "1390 2.3807532787323 2.3807532787323\n",
      "1400 2.3737030029296875 2.3737030029296875\n",
      "1410 2.374337673187256 2.374337673187256\n",
      "1420 2.4673588275909424 2.4673588275909424\n",
      "1430 2.3353793621063232 2.3353793621063232\n",
      "1440 2.3755381107330322 2.3755381107330322\n",
      "1450 2.3929386138916016 2.3929386138916016\n",
      "1460 2.3307831287384033 2.3307831287384033\n",
      "1470 2.505056142807007 2.505056142807007\n",
      "1480 2.4222450256347656 2.4222450256347656\n",
      "1490 2.3458235263824463 2.3458235263824463\n",
      "1500 2.34694504737854 2.34694504737854\n",
      "1510 2.5538976192474365 2.5538976192474365\n",
      "1520 2.289332628250122 2.289332628250122\n",
      "1530 2.3147735595703125 2.3147735595703125\n",
      "1540 2.301316738128662 2.301316738128662\n",
      "1550 2.3970603942871094 2.3970603942871094\n",
      "1560 2.509176731109619 2.509176731109619\n",
      "1570 2.4577174186706543 2.4577174186706543\n",
      "1580 2.3616440296173096 2.3616440296173096\n",
      "1590 2.1906981468200684 2.1906981468200684\n",
      "1600 2.2478301525115967 2.2478301525115967\n",
      "1610 2.3665332794189453 2.3665332794189453\n",
      "1620 2.3300466537475586 2.3300466537475586\n",
      "1630 2.540503740310669 2.540503740310669\n",
      "1640 2.4993767738342285 2.4993767738342285\n",
      "1650 2.3721842765808105 2.3721842765808105\n",
      "1660 2.3232483863830566 2.3232483863830566\n",
      "1670 2.483691692352295 2.483691692352295\n",
      "1680 2.25620698928833 2.25620698928833\n",
      "1690 2.2304835319519043 2.2304835319519043\n",
      "1700 2.307896852493286 2.307896852493286\n",
      "1710 2.3961308002471924 2.3961308002471924\n",
      "1720 2.3562655448913574 2.3562655448913574\n",
      "1730 2.3742594718933105 2.3742594718933105\n",
      "1740 2.303551435470581 2.303551435470581\n",
      "1750 2.2504513263702393 2.2504513263702393\n",
      "1760 2.340226650238037 2.340226650238037\n",
      "1770 2.2358884811401367 2.2358884811401367\n",
      "1780 2.19575834274292 2.19575834274292\n",
      "1790 2.4452829360961914 2.4452829360961914\n",
      "1800 2.3224411010742188 2.3224411010742188\n",
      "1810 2.2122642993927 2.2122642993927\n",
      "1820 2.3308331966400146 2.3308331966400146\n",
      "1830 2.326850175857544 2.326850175857544\n",
      "1840 2.4416074752807617 2.4416074752807617\n",
      "1850 2.4605610370635986 2.4605610370635986\n",
      "1860 2.419806718826294 2.419806718826294\n",
      "1870 2.3110032081604004 2.3110032081604004\n",
      "1880 2.280151128768921 2.280151128768921\n",
      "1890 2.440066337585449 2.440066337585449\n",
      "1900 2.5049285888671875 2.5049285888671875\n",
      "1910 2.358013391494751 2.358013391494751\n",
      "1920 2.5140159130096436 2.5140159130096436\n",
      "1930 2.351905107498169 2.351905107498169\n",
      "1940 2.3157989978790283 2.3157989978790283\n",
      "1950 2.1924057006835938 2.1924057006835938\n",
      "1960 2.326977014541626 2.326977014541626\n",
      "1970 2.401237726211548 2.401237726211548\n",
      "1980 2.352437734603882 2.352437734603882\n",
      "1990 2.3138186931610107 2.3138186931610107\n",
      "2000 2.3561112880706787 2.3561112880706787\n",
      "2010 2.3151814937591553 2.3151814937591553\n",
      "2020 2.2912473678588867 2.2912473678588867\n",
      "2030 2.4588427543640137 2.4588427543640137\n",
      "2040 2.510820150375366 2.510820150375366\n",
      "2050 2.351353645324707 2.351353645324707\n",
      "2060 2.346151828765869 2.346151828765869\n",
      "2070 2.4041836261749268 2.4041836261749268\n",
      "2080 2.37606143951416 2.37606143951416\n",
      "2090 2.2991716861724854 2.2991716861724854\n",
      "2100 2.387971878051758 2.387971878051758\n",
      "2110 2.3783645629882812 2.3783645629882812\n",
      "2120 2.340862274169922 2.340862274169922\n",
      "2130 2.2721219062805176 2.2721219062805176\n",
      "2140 2.1813480854034424 2.1813480854034424\n",
      "2150 2.294755697250366 2.294755697250366\n",
      "2160 2.4751763343811035 2.4751763343811035\n",
      "2170 2.356144666671753 2.356144666671753\n",
      "2180 2.3841326236724854 2.3841326236724854\n",
      "2190 2.389263153076172 2.389263153076172\n",
      "2200 2.291492223739624 2.291492223739624\n",
      "2210 2.37146258354187 2.37146258354187\n",
      "2220 2.4083006381988525 2.4083006381988525\n",
      "2230 2.4508183002471924 2.4508183002471924\n",
      "2240 2.3679962158203125 2.3679962158203125\n",
      "2250 2.4095942974090576 2.4095942974090576\n",
      "2260 2.4996774196624756 2.4996774196624756\n",
      "2270 2.459850311279297 2.459850311279297\n",
      "2280 2.324549913406372 2.324549913406372\n",
      "2290 2.382983684539795 2.382983684539795\n",
      "2300 2.1943817138671875 2.1943817138671875\n",
      "2310 2.293377637863159 2.293377637863159\n",
      "2320 2.3917195796966553 2.3917195796966553\n",
      "2330 2.340817928314209 2.340817928314209\n",
      "2340 2.3595192432403564 2.3595192432403564\n",
      "2350 2.230891704559326 2.230891704559326\n",
      "2360 2.4262304306030273 2.4262304306030273\n",
      "2370 2.2889201641082764 2.2889201641082764\n",
      "2380 2.2051963806152344 2.2051963806152344\n",
      "2390 2.243683338165283 2.243683338165283\n",
      "2400 2.33931040763855 2.33931040763855\n",
      "2410 2.4158313274383545 2.4158313274383545\n",
      "2420 2.4157488346099854 2.4157488346099854\n",
      "2430 2.217163324356079 2.217163324356079\n",
      "2440 2.269404649734497 2.269404649734497\n",
      "2450 2.2758960723876953 2.2758960723876953\n",
      "2460 2.4662275314331055 2.4662275314331055\n",
      "2470 2.3508918285369873 2.3508918285369873\n",
      "2480 2.3370542526245117 2.3370542526245117\n",
      "2490 2.464541435241699 2.464541435241699\n",
      "2500 2.2299187183380127 2.2299187183380127\n",
      "2510 2.2517495155334473 2.2517495155334473\n",
      "2520 2.139922618865967 2.139922618865967\n",
      "2530 2.3450636863708496 2.3450636863708496\n",
      "2540 2.484557867050171 2.484557867050171\n",
      "2550 2.342444658279419 2.342444658279419\n",
      "2560 2.2130627632141113 2.2130627632141113\n",
      "2570 2.3646442890167236 2.3646442890167236\n",
      "2580 2.450105667114258 2.450105667114258\n",
      "2590 2.4114990234375 2.4114990234375\n",
      "2600 2.2809321880340576 2.2809321880340576\n",
      "2610 2.1955718994140625 2.1955718994140625\n",
      "2620 2.3205981254577637 2.3205981254577637\n",
      "2630 2.2609922885894775 2.2609922885894775\n",
      "2640 2.3856678009033203 2.3856678009033203\n",
      "2650 2.378547430038452 2.378547430038452\n",
      "2660 2.3168368339538574 2.3168368339538574\n",
      "2670 2.289985418319702 2.289985418319702\n",
      "2680 2.4347283840179443 2.4347283840179443\n",
      "2690 2.2711105346679688 2.2711105346679688\n",
      "2700 2.373253107070923 2.373253107070923\n",
      "2710 2.3336331844329834 2.3336331844329834\n",
      "2720 2.293078899383545 2.293078899383545\n",
      "2730 2.2951018810272217 2.2951018810272217\n",
      "2740 2.2323102951049805 2.2323102951049805\n",
      "2750 2.3908743858337402 2.3908743858337402\n",
      "2760 2.3746817111968994 2.3746817111968994\n",
      "2770 2.344238042831421 2.344238042831421\n",
      "2780 2.297168731689453 2.297168731689453\n",
      "2790 2.265392303466797 2.265392303466797\n",
      "2800 2.373173713684082 2.373173713684082\n",
      "2810 2.402921199798584 2.402921199798584\n",
      "2820 2.340930223464966 2.340930223464966\n",
      "2830 2.2931482791900635 2.2931482791900635\n",
      "2840 2.23272967338562 2.23272967338562\n",
      "2850 2.3048787117004395 2.3048787117004395\n",
      "2860 2.1841561794281006 2.1841561794281006\n",
      "2870 2.3135318756103516 2.3135318756103516\n",
      "2880 2.3917887210845947 2.3917887210845947\n",
      "2890 2.3812201023101807 2.3812201023101807\n",
      "2900 2.220834255218506 2.220834255218506\n",
      "2910 2.076317071914673 2.076317071914673\n",
      "2920 2.368263006210327 2.368263006210327\n",
      "2930 2.538257360458374 2.538257360458374\n",
      "2940 2.355971574783325 2.355971574783325\n",
      "2950 2.2904775142669678 2.2904775142669678\n",
      "2960 2.4597156047821045 2.4597156047821045\n",
      "2970 2.355442762374878 2.355442762374878\n",
      "2980 2.2188186645507812 2.2188186645507812\n",
      "2990 2.3238117694854736 2.3238117694854736\n",
      "3000 2.3508875370025635 2.3508875370025635\n",
      "3010 2.3615143299102783 2.3615143299102783\n",
      "3020 2.1741340160369873 2.1741340160369873\n",
      "3030 2.323107957839966 2.323107957839966\n",
      "3040 2.3420004844665527 2.3420004844665527\n",
      "3050 2.1651811599731445 2.1651811599731445\n",
      "3060 2.423572063446045 2.423572063446045\n",
      "3070 2.447093963623047 2.447093963623047\n",
      "3080 2.2205841541290283 2.2205841541290283\n",
      "3090 2.26992130279541 2.26992130279541\n",
      "3100 2.246114492416382 2.246114492416382\n",
      "3110 2.2358198165893555 2.2358198165893555\n",
      "3120 2.279526948928833 2.279526948928833\n",
      "3130 2.4005191326141357 2.4005191326141357\n",
      "3140 2.408926010131836 2.408926010131836\n",
      "3150 2.3526599407196045 2.3526599407196045\n",
      "3160 2.251650333404541 2.251650333404541\n",
      "3170 2.1823997497558594 2.1823997497558594\n",
      "3180 2.323395013809204 2.323395013809204\n",
      "3190 2.191868305206299 2.191868305206299\n",
      "3200 2.22696590423584 2.22696590423584\n",
      "3210 2.2150065898895264 2.2150065898895264\n",
      "3220 2.214311122894287 2.214311122894287\n",
      "3230 2.2626254558563232 2.2626254558563232\n",
      "3240 2.2889466285705566 2.2889466285705566\n",
      "3250 2.2732796669006348 2.2732796669006348\n",
      "3260 2.2179722785949707 2.2179722785949707\n",
      "3270 2.4308924674987793 2.4308924674987793\n",
      "3280 2.253614664077759 2.253614664077759\n",
      "3290 2.3649797439575195 2.3649797439575195\n",
      "3300 2.415358066558838 2.415358066558838\n",
      "3310 2.2924258708953857 2.2924258708953857\n",
      "3320 2.3495497703552246 2.3495497703552246\n",
      "3330 2.2630956172943115 2.2630956172943115\n",
      "3340 2.260847330093384 2.260847330093384\n",
      "3350 2.2113921642303467 2.2113921642303467\n",
      "3360 2.1488213539123535 2.1488213539123535\n",
      "3370 2.274059772491455 2.274059772491455\n",
      "3380 2.246901750564575 2.246901750564575\n",
      "3390 2.3349997997283936 2.3349997997283936\n",
      "3400 2.378263473510742 2.378263473510742\n",
      "3410 2.25531268119812 2.25531268119812\n",
      "3420 2.251877546310425 2.251877546310425\n",
      "3430 2.342557668685913 2.342557668685913\n",
      "3440 2.2826318740844727 2.2826318740844727\n",
      "3450 2.2172553539276123 2.2172553539276123\n",
      "3460 2.212548017501831 2.212548017501831\n",
      "3470 2.110060930252075 2.110060930252075\n",
      "3480 2.2841498851776123 2.2841498851776123\n",
      "3490 2.3875365257263184 2.3875365257263184\n",
      "3500 2.259270191192627 2.259270191192627\n",
      "3510 2.145336866378784 2.145336866378784\n",
      "3520 2.159662961959839 2.159662961959839\n",
      "3530 2.303642988204956 2.303642988204956\n",
      "3540 2.2454373836517334 2.2454373836517334\n",
      "3550 2.224358558654785 2.224358558654785\n",
      "3560 2.2563705444335938 2.2563705444335938\n",
      "3570 2.1871559619903564 2.1871559619903564\n",
      "3580 2.185743570327759 2.185743570327759\n",
      "3590 2.2646126747131348 2.2646126747131348\n",
      "3600 2.281433343887329 2.281433343887329\n",
      "3610 2.268213987350464 2.268213987350464\n",
      "3620 2.365206003189087 2.365206003189087\n",
      "3630 2.4272897243499756 2.4272897243499756\n",
      "3640 2.308297634124756 2.308297634124756\n",
      "3650 2.235874891281128 2.235874891281128\n",
      "3660 2.108125925064087 2.108125925064087\n",
      "3670 2.3473546504974365 2.3473546504974365\n",
      "3680 2.217834234237671 2.217834234237671\n",
      "3690 2.2781317234039307 2.2781317234039307\n",
      "3700 2.176805257797241 2.176805257797241\n",
      "3710 2.1250290870666504 2.1250290870666504\n",
      "3720 2.124690055847168 2.124690055847168\n",
      "3730 2.2286758422851562 2.2286758422851562\n",
      "3740 2.1654536724090576 2.1654536724090576\n",
      "3750 2.2465529441833496 2.2465529441833496\n",
      "3760 2.390263795852661 2.390263795852661\n",
      "3770 2.2554547786712646 2.2554547786712646\n",
      "3780 2.200881004333496 2.200881004333496\n",
      "3790 2.2909696102142334 2.2909696102142334\n",
      "3800 2.3507766723632812 2.3507766723632812\n",
      "3810 2.1046955585479736 2.1046955585479736\n",
      "3820 2.318723678588867 2.318723678588867\n",
      "3830 2.2488279342651367 2.2488279342651367\n",
      "3840 2.3368043899536133 2.3368043899536133\n",
      "3850 2.300724506378174 2.300724506378174\n",
      "3860 2.4125494956970215 2.4125494956970215\n",
      "3870 2.3068456649780273 2.3068456649780273\n",
      "3880 2.1034488677978516 2.1034488677978516\n",
      "3890 2.147698402404785 2.147698402404785\n",
      "3900 2.282353162765503 2.282353162765503\n",
      "3910 2.093484878540039 2.093484878540039\n",
      "3920 2.428788423538208 2.428788423538208\n",
      "3930 2.202953815460205 2.202953815460205\n",
      "3940 2.1851086616516113 2.1851086616516113\n",
      "3950 2.1720330715179443 2.1720330715179443\n",
      "3960 2.1695706844329834 2.1695706844329834\n",
      "3970 2.2201988697052 2.2201988697052\n",
      "3980 2.3675992488861084 2.3675992488861084\n",
      "3990 2.3043599128723145 2.3043599128723145\n",
      "4000 2.1530635356903076 2.1530635356903076\n",
      "4010 2.2605602741241455 2.2605602741241455\n",
      "4020 2.1916651725769043 2.1916651725769043\n",
      "4030 2.3016674518585205 2.3016674518585205\n",
      "4040 2.1831443309783936 2.1831443309783936\n",
      "4050 2.1185333728790283 2.1185333728790283\n",
      "4060 2.314974308013916 2.314974308013916\n",
      "4070 2.244037628173828 2.244037628173828\n",
      "4080 2.2834744453430176 2.2834744453430176\n",
      "4090 2.2216053009033203 2.2216053009033203\n",
      "4100 2.3077685832977295 2.3077685832977295\n",
      "4110 2.1747608184814453 2.1747608184814453\n",
      "4120 2.2884695529937744 2.2884695529937744\n",
      "4130 2.293133020401001 2.293133020401001\n",
      "4140 2.2525506019592285 2.2525506019592285\n",
      "4150 2.272615671157837 2.272615671157837\n",
      "4160 2.4760396480560303 2.4760396480560303\n",
      "4170 2.1031911373138428 2.1031911373138428\n",
      "4180 2.187991142272949 2.187991142272949\n",
      "4190 2.378727912902832 2.378727912902832\n",
      "4200 2.261131763458252 2.261131763458252\n",
      "4210 2.2687902450561523 2.2687902450561523\n",
      "4220 2.3434901237487793 2.3434901237487793\n",
      "4230 2.2721729278564453 2.2721729278564453\n",
      "4240 2.327787160873413 2.327787160873413\n",
      "4250 2.167607307434082 2.167607307434082\n",
      "4260 2.3291711807250977 2.3291711807250977\n",
      "4270 2.321613073348999 2.321613073348999\n",
      "4280 2.367138147354126 2.367138147354126\n",
      "4290 2.2786340713500977 2.2786340713500977\n",
      "4300 2.31892466545105 2.31892466545105\n",
      "4310 2.28885817527771 2.28885817527771\n",
      "4320 2.181122303009033 2.181122303009033\n",
      "4330 2.2112464904785156 2.2112464904785156\n",
      "4340 2.346787452697754 2.346787452697754\n",
      "4350 2.1654646396636963 2.1654646396636963\n",
      "4360 2.242398262023926 2.242398262023926\n",
      "4370 2.3858144283294678 2.3858144283294678\n",
      "4380 2.275650978088379 2.275650978088379\n",
      "4390 2.2986819744110107 2.2986819744110107\n",
      "4400 2.1980671882629395 2.1980671882629395\n",
      "4410 2.373016834259033 2.373016834259033\n",
      "4420 2.188434362411499 2.188434362411499\n",
      "4430 2.1151578426361084 2.1151578426361084\n",
      "4440 2.1621875762939453 2.1621875762939453\n",
      "4450 2.140432357788086 2.140432357788086\n",
      "4460 2.1271724700927734 2.1271724700927734\n",
      "4470 2.283910036087036 2.283910036087036\n",
      "4480 2.242300271987915 2.242300271987915\n",
      "4490 2.23684024810791 2.23684024810791\n",
      "4500 2.2211074829101562 2.2211074829101562\n",
      "4510 2.358609199523926 2.358609199523926\n",
      "4520 2.2463462352752686 2.2463462352752686\n",
      "4530 2.217282772064209 2.217282772064209\n",
      "4540 2.095064640045166 2.095064640045166\n",
      "4550 2.094292163848877 2.094292163848877\n",
      "4560 2.075137138366699 2.075137138366699\n",
      "4570 2.3552463054656982 2.3552463054656982\n",
      "4580 2.2860300540924072 2.2860300540924072\n",
      "4590 2.201280117034912 2.201280117034912\n",
      "4600 2.1214663982391357 2.1214663982391357\n",
      "4610 2.2340517044067383 2.2340517044067383\n",
      "4620 2.0918209552764893 2.0918209552764893\n",
      "4630 2.07368540763855 2.07368540763855\n",
      "4640 2.2828922271728516 2.2828922271728516\n",
      "4650 2.2542757987976074 2.2542757987976074\n",
      "4660 2.212519884109497 2.212519884109497\n",
      "4670 2.192584991455078 2.192584991455078\n",
      "4680 2.274120807647705 2.274120807647705\n",
      "4690 2.069638252258301 2.069638252258301\n",
      "4700 2.2099947929382324 2.2099947929382324\n",
      "4710 2.3074076175689697 2.3074076175689697\n",
      "4720 2.3552567958831787 2.3552567958831787\n",
      "4730 2.2576446533203125 2.2576446533203125\n",
      "4740 2.0876822471618652 2.0876822471618652\n",
      "4750 2.1047487258911133 2.1047487258911133\n",
      "4760 2.3843259811401367 2.3843259811401367\n",
      "4770 2.308255910873413 2.308255910873413\n",
      "4780 2.270037889480591 2.270037889480591\n",
      "4790 2.0089111328125 2.0089111328125\n",
      "4800 2.1709470748901367 2.1709470748901367\n",
      "4810 2.153409719467163 2.153409719467163\n",
      "4820 2.3281726837158203 2.3281726837158203\n",
      "4830 2.218808174133301 2.218808174133301\n",
      "4840 2.3994314670562744 2.3994314670562744\n",
      "4850 2.3192811012268066 2.3192811012268066\n",
      "4860 2.290189266204834 2.290189266204834\n",
      "4870 2.2985756397247314 2.2985756397247314\n",
      "4880 2.1458778381347656 2.1458778381347656\n",
      "4890 2.241619825363159 2.241619825363159\n",
      "4900 2.2803196907043457 2.2803196907043457\n",
      "4910 2.326935291290283 2.326935291290283\n",
      "4920 2.086678981781006 2.086678981781006\n",
      "4930 2.313706159591675 2.313706159591675\n",
      "4940 2.2499356269836426 2.2499356269836426\n",
      "4950 2.230942726135254 2.230942726135254\n",
      "4960 2.228942632675171 2.228942632675171\n",
      "4970 2.279066324234009 2.279066324234009\n",
      "4980 2.28250789642334 2.28250789642334\n",
      "4990 2.2906742095947266 2.2906742095947266\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T10:27:44.385918Z",
     "start_time": "2025-12-05T10:27:43.905156Z"
    }
   },
   "cell_type": "code",
   "source": " print(decode(m.generate(idx, max_new_tokens=300)[0].tolist()))",
   "id": "fd0eea4b0a92a9aa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Whent ther,\n",
      "Thow and is by be madisen bobe toe.\n",
      "Sagruans me?\n",
      "I and bar hapaing qou he.\n",
      "War dilth anes wice my.\n",
      "\n",
      "HDER:\n",
      "An onou thouns, to tis he me mil; dill, bes istees, hain lat Her drove thand qure now ond wabmes lelind me letlis my now by:\n",
      "Save aisse, why. Hll mal ky mopetelaves\n",
      "Momery wor moth k\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Scaled model with layer norm and residual connections.",
   "id": "11e93917e3dfe81d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T10:27:44.566063Z",
     "start_time": "2025-12-05T10:27:44.538703Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "#defining hyperparameters\n",
    "batch_size = 32\n",
    "block_size = 128\n",
    "learning_rate = 3e-4\n",
    "n_layer = 6\n",
    "n_heads = 6\n",
    "n_embd = 384\n",
    "dropout = 0.2\n",
    "vocab_size = len(chars)\n",
    "\n",
    "\n",
    "#implelementing single head self attention as a nn.Module\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)  #key , what to look for\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)  #query , what to match with\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)  #value , what to return\n",
    "        self.register_buffer('tril',\n",
    "                             torch.tril(torch.ones(block_size, block_size)))  #lower triangular matrix for masking\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)  # BXTXhead_size\n",
    "        q = self.query(x)  # BXTXhead_size\n",
    "        wei = q @ k.transpose(-2, -1) * (k.shape[-1] ** -0.5)  #scaling factor , for softmax stability .\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)  #softmax along the rows\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)  # BXTXhead_size\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        #create a list of multiple heads.\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(num_heads * head_size, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #concatenate the output of each head along the channel dimension , basiclly stacking them.\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout)  #adding dropout for regularization\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, num_heads):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // num_heads\n",
    "        self.sa = MultiHeadAttention(num_heads, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))  #residual connection , normalize before transforming.\n",
    "        x = x + self.ffwd(self.ln2(x))  #residual connection , normalize before transforming.\n",
    "        return x\n",
    "\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        #each row represents a token id , and each mat[i,j] is the score for jth token in the vocab given ith token.\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)  #for each token we get a vector its repr'.\n",
    "        self.position_embedding_table = nn.Embedding(block_size,\n",
    "                                                     n_embd)  #repr' of the position of the token in the sequence.\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(n_embd, num_heads=n_heads) for _ in range(n_layer)])  #stacking multiple blocks\n",
    "        self.ln_f = nn.LayerNorm(n_embd)  #final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        #idx is the BXT tensor of input token indices .\n",
    "        token_emb = self.token_embedding_table(idx)  #BXTXC.\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  #TXC  .\n",
    "        x = token_emb + pos_emb  #BXTXC , adding token embedding and position embedding.\n",
    "        x = self.blocks(x)  #applying self attention , making the x tokens enriched with context from previous tokens.\n",
    "        x = self.ln_f(x)  #applying feed forward layer.\n",
    "        logits = self.lm_head(x)  #BXTXvocab_size\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)  # (BXT)XC. reshaping for loss computation 3d to 2d.\n",
    "            targets = targets.view(B * T)  # (BXT) . reshaping for loss computation 2d to 1d.\n",
    "            #how well we are predicting the targets\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            #for each entry we will return the logits for the next token.\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        #idx is a BXT tensor of input token\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]  #conditioning only on the last block_size tokens. for now 2.\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]  #BXC  , for each batch we take the predictions for the last token.\n",
    "            probs = F.softmax(logits, dim=-1)  #BXC , converting to probabilities\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  #BX1 , the next predicted token for each batch.\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # BX(T+1) , appending the predicted token to the input sequence.\n",
    "        return idx\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "b2c6930e13b04cca",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T10:39:54.198655Z",
     "start_time": "2025-12-05T10:27:44.579547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "m = BigramLanguageModel().to(device)\n",
    "out, loss = m(xb, yb)\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=0.001)\n",
    "#training the model , as before.\n",
    "batch_size = 32  #how many independent sequences will we process in parallel\n",
    "for steps in range(5000):\n",
    "    #get a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    xb_val, yb_val = get_batch('val')\n",
    "    #evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    _, val_loss = m(xb_val, yb_val)\n",
    "    #backpropagation\n",
    "    optimizer.zero_grad(set_to_none=True)  #set gradients to zero , to avoid accumulation from previous step.\n",
    "    loss.backward()  # compute gradients , see how much each weight contributed to the loss.\n",
    "    optimizer.step()\n",
    "    if steps % 10 == 0:\n",
    "        print(steps, loss.item(), val_loss.item())"
   ],
   "id": "bcb1d080e706bec8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.284030914306641 4.306404113769531\n",
      "10 3.2611048221588135 3.2826149463653564\n",
      "20 2.92818284034729 2.9606494903564453\n",
      "30 2.699735164642334 2.7456071376800537\n",
      "40 2.6365113258361816 2.601130962371826\n",
      "50 2.595243215560913 2.6004798412323\n",
      "60 2.5030527114868164 2.538848400115967\n",
      "70 2.5128283500671387 2.5221610069274902\n",
      "80 2.520505905151367 2.480175733566284\n",
      "90 2.4648663997650146 2.5371363162994385\n",
      "100 2.47782826423645 2.4564924240112305\n",
      "110 2.5109434127807617 2.4575448036193848\n",
      "120 2.459712266921997 2.451277017593384\n",
      "130 2.419159173965454 2.416468858718872\n",
      "140 2.380516529083252 2.429774045944214\n",
      "150 2.4079182147979736 2.3904664516448975\n",
      "160 2.388704299926758 2.4304099082946777\n",
      "170 2.340021848678589 2.3888797760009766\n",
      "180 2.3133304119110107 2.330843448638916\n",
      "190 2.236462354660034 2.2844462394714355\n",
      "200 2.2527027130126953 2.2857232093811035\n",
      "210 2.2185773849487305 2.276191234588623\n",
      "220 2.125089406967163 2.235943555831909\n",
      "230 2.1827735900878906 2.189829111099243\n",
      "240 2.165018320083618 2.2072277069091797\n",
      "250 2.119779586791992 2.132476568222046\n",
      "260 2.079221487045288 2.1627914905548096\n",
      "270 2.1511831283569336 2.1805989742279053\n",
      "280 2.086103677749634 2.1551454067230225\n",
      "290 2.0486207008361816 2.149162769317627\n",
      "300 2.0842502117156982 2.1056556701660156\n",
      "310 1.999694585800171 2.0362534523010254\n",
      "320 2.0093536376953125 2.1114468574523926\n",
      "330 1.92728853225708 2.055445909500122\n",
      "340 1.988802433013916 2.114339828491211\n",
      "350 2.0218276977539062 2.0709052085876465\n",
      "360 1.9743328094482422 2.05903697013855\n",
      "370 1.932047724723816 2.013075113296509\n",
      "380 1.8896582126617432 2.046996831893921\n",
      "390 1.9427130222320557 2.0179548263549805\n",
      "400 1.908075213432312 2.0323429107666016\n",
      "410 1.8546879291534424 2.0382983684539795\n",
      "420 1.8370106220245361 1.9727498292922974\n",
      "430 1.8966549634933472 1.990643858909607\n",
      "440 1.8826792240142822 1.9847493171691895\n",
      "450 1.8223234415054321 1.9395004510879517\n",
      "460 1.7992111444473267 1.9658615589141846\n",
      "470 1.785441279411316 1.9977296590805054\n",
      "480 1.789292573928833 1.9839458465576172\n",
      "490 1.7720744609832764 1.9290825128555298\n",
      "500 1.8317797183990479 1.9559489488601685\n",
      "510 1.801139235496521 1.9244332313537598\n",
      "520 1.7948778867721558 1.9209983348846436\n",
      "530 1.7813925743103027 1.9699971675872803\n",
      "540 1.6563057899475098 1.8686443567276\n",
      "550 1.6788326501846313 1.9247187376022339\n",
      "560 1.7333199977874756 1.9475599527359009\n",
      "570 1.7158329486846924 1.8915398120880127\n",
      "580 1.6961125135421753 1.8782575130462646\n",
      "590 1.6580586433410645 1.9023760557174683\n",
      "600 1.74312424659729 1.8809154033660889\n",
      "610 1.740544319152832 1.8788707256317139\n",
      "620 1.7108606100082397 1.8320250511169434\n",
      "630 1.6356712579727173 1.8470124006271362\n",
      "640 1.6434071063995361 1.9268667697906494\n",
      "650 1.6743507385253906 1.8548983335494995\n",
      "660 1.688732385635376 1.7799570560455322\n",
      "670 1.6657471656799316 1.8208634853363037\n",
      "680 1.6466500759124756 1.8070927858352661\n",
      "690 1.6853036880493164 1.812982439994812\n",
      "700 1.6166197061538696 1.8585809469223022\n",
      "710 1.5800954103469849 1.8198885917663574\n",
      "720 1.633226752281189 1.77450692653656\n",
      "730 1.6565111875534058 1.8052881956100464\n",
      "740 1.6073275804519653 1.8487666845321655\n",
      "750 1.5973126888275146 1.834343433380127\n",
      "760 1.6081666946411133 1.8025121688842773\n",
      "770 1.614908218383789 1.7558122873306274\n",
      "780 1.5880944728851318 1.8168189525604248\n",
      "790 1.5984169244766235 1.8521331548690796\n",
      "800 1.5604431629180908 1.7419121265411377\n",
      "810 1.5797336101531982 1.7600797414779663\n",
      "820 1.6014916896820068 1.7263110876083374\n",
      "830 1.550391435623169 1.8030905723571777\n",
      "840 1.5612525939941406 1.8267786502838135\n",
      "850 1.618435263633728 1.7854093313217163\n",
      "860 1.5574767589569092 1.7407296895980835\n",
      "870 1.6237419843673706 1.7579433917999268\n",
      "880 1.6035137176513672 1.7874759435653687\n",
      "890 1.5894232988357544 1.7580349445343018\n",
      "900 1.5796446800231934 1.7819468975067139\n",
      "910 1.6043095588684082 1.78376042842865\n",
      "920 1.5312902927398682 1.7006586790084839\n",
      "930 1.5396636724472046 1.8060722351074219\n",
      "940 1.5336942672729492 1.7180196046829224\n",
      "950 1.549336552619934 1.724190592765808\n",
      "960 1.5660899877548218 1.7540041208267212\n",
      "970 1.561767339706421 1.7743861675262451\n",
      "980 1.5669499635696411 1.71043062210083\n",
      "990 1.5058887004852295 1.7156039476394653\n",
      "1000 1.5435062646865845 1.6930521726608276\n",
      "1010 1.556941270828247 1.7095624208450317\n",
      "1020 1.541823387145996 1.6896594762802124\n",
      "1030 1.623326301574707 1.7943731546401978\n",
      "1040 1.4989322423934937 1.6769373416900635\n",
      "1050 1.5725973844528198 1.68068265914917\n",
      "1060 1.5516337156295776 1.7061854600906372\n",
      "1070 1.5299879312515259 1.6756938695907593\n",
      "1080 1.5426726341247559 1.6653984785079956\n",
      "1090 1.5291439294815063 1.7529398202896118\n",
      "1100 1.5040867328643799 1.7265002727508545\n",
      "1110 1.4956386089324951 1.6282145977020264\n",
      "1120 1.5084474086761475 1.6605634689331055\n",
      "1130 1.5023057460784912 1.7127124071121216\n",
      "1140 1.5024042129516602 1.7238144874572754\n",
      "1150 1.4808796644210815 1.6955050230026245\n",
      "1160 1.565025806427002 1.6648608446121216\n",
      "1170 1.5230838060379028 1.6197211742401123\n",
      "1180 1.4788743257522583 1.679322361946106\n",
      "1190 1.4835227727890015 1.7191914319992065\n",
      "1200 1.5195127725601196 1.6134681701660156\n",
      "1210 1.4544146060943604 1.686694622039795\n",
      "1220 1.4696718454360962 1.6900333166122437\n",
      "1230 1.4779481887817383 1.718583345413208\n",
      "1240 1.503912091255188 1.6820842027664185\n",
      "1250 1.5333069562911987 1.6700685024261475\n",
      "1260 1.5074406862258911 1.65805983543396\n",
      "1270 1.5159382820129395 1.6289417743682861\n",
      "1280 1.5130048990249634 1.6575813293457031\n",
      "1290 1.507509469985962 1.6786234378814697\n",
      "1300 1.4554686546325684 1.6368952989578247\n",
      "1310 1.5166089534759521 1.6433043479919434\n",
      "1320 1.4867334365844727 1.6236456632614136\n",
      "1330 1.459214687347412 1.572049617767334\n",
      "1340 1.3668699264526367 1.5919393301010132\n",
      "1350 1.3815181255340576 1.7076189517974854\n",
      "1360 1.4895546436309814 1.6494202613830566\n",
      "1370 1.4662634134292603 1.6407216787338257\n",
      "1380 1.469402551651001 1.55825936794281\n",
      "1390 1.449903964996338 1.659342885017395\n",
      "1400 1.5033905506134033 1.5849748849868774\n",
      "1410 1.4523186683654785 1.6989986896514893\n",
      "1420 1.470686912536621 1.64836847782135\n",
      "1430 1.4201492071151733 1.6529184579849243\n",
      "1440 1.3998570442199707 1.630157470703125\n",
      "1450 1.4433764219284058 1.6531858444213867\n",
      "1460 1.3664573431015015 1.642162799835205\n",
      "1470 1.3606197834014893 1.6516364812850952\n",
      "1480 1.4440451860427856 1.7337981462478638\n",
      "1490 1.4479374885559082 1.6325526237487793\n",
      "1500 1.4298523664474487 1.5613492727279663\n",
      "1510 1.4213078022003174 1.630091905593872\n",
      "1520 1.4462801218032837 1.5923267602920532\n",
      "1530 1.455488681793213 1.531388521194458\n",
      "1540 1.42281973361969 1.6566963195800781\n",
      "1550 1.4231812953948975 1.5936354398727417\n",
      "1560 1.5039070844650269 1.6335080862045288\n",
      "1570 1.4311121702194214 1.6108454465866089\n",
      "1580 1.4581977128982544 1.5955426692962646\n",
      "1590 1.4124398231506348 1.643220067024231\n",
      "1600 1.4230629205703735 1.6061577796936035\n",
      "1610 1.4388631582260132 1.6712756156921387\n",
      "1620 1.4447994232177734 1.697095513343811\n",
      "1630 1.4294233322143555 1.5786826610565186\n",
      "1640 1.450505018234253 1.606650948524475\n",
      "1650 1.4070900678634644 1.670603632926941\n",
      "1660 1.3896292448043823 1.6874949932098389\n",
      "1670 1.407020926475525 1.5543084144592285\n",
      "1680 1.4105421304702759 1.724369764328003\n",
      "1690 1.4248316287994385 1.622554898262024\n",
      "1700 1.4728710651397705 1.5785430669784546\n",
      "1710 1.3806171417236328 1.5843470096588135\n",
      "1720 1.3767050504684448 1.5226022005081177\n",
      "1730 1.4229038953781128 1.5715980529785156\n",
      "1740 1.4222124814987183 1.5683623552322388\n",
      "1750 1.4478527307510376 1.632009506225586\n",
      "1760 1.3937809467315674 1.7034938335418701\n",
      "1770 1.353783130645752 1.6838634014129639\n",
      "1780 1.3771847486495972 1.659949541091919\n",
      "1790 1.3786606788635254 1.625618577003479\n",
      "1800 1.3944201469421387 1.6132102012634277\n",
      "1810 1.3600236177444458 1.6013991832733154\n",
      "1820 1.4175903797149658 1.6315642595291138\n",
      "1830 1.373202919960022 1.6060075759887695\n",
      "1840 1.3763333559036255 1.6242152452468872\n",
      "1850 1.2813827991485596 1.6302739381790161\n",
      "1860 1.3817167282104492 1.5686366558074951\n",
      "1870 1.4183032512664795 1.571740984916687\n",
      "1880 1.3985646963119507 1.5461006164550781\n",
      "1890 1.3934509754180908 1.5947580337524414\n",
      "1900 1.3644134998321533 1.6255701780319214\n",
      "1910 1.322739839553833 1.5528700351715088\n",
      "1920 1.3797308206558228 1.6392323970794678\n",
      "1930 1.4161947965621948 1.5436989068984985\n",
      "1940 1.3797954320907593 1.673391342163086\n",
      "1950 1.3883459568023682 1.58266282081604\n",
      "1960 1.3680918216705322 1.6021919250488281\n",
      "1970 1.4060102701187134 1.4831432104110718\n",
      "1980 1.35141921043396 1.565969467163086\n",
      "1990 1.3387930393218994 1.6194353103637695\n",
      "2000 1.3769810199737549 1.6192553043365479\n",
      "2010 1.3570737838745117 1.598252296447754\n",
      "2020 1.3440563678741455 1.5677241086959839\n",
      "2030 1.3360577821731567 1.5966119766235352\n",
      "2040 1.3657690286636353 1.5496569871902466\n",
      "2050 1.3733488321304321 1.585978388786316\n",
      "2060 1.384924292564392 1.5200990438461304\n",
      "2070 1.3750637769699097 1.5925846099853516\n",
      "2080 1.361240029335022 1.6028449535369873\n",
      "2090 1.4644420146942139 1.598045825958252\n",
      "2100 1.3540462255477905 1.5826343297958374\n",
      "2110 1.357128620147705 1.5914486646652222\n",
      "2120 1.3317136764526367 1.5295922756195068\n",
      "2130 1.4095808267593384 1.637861728668213\n",
      "2140 1.3568735122680664 1.5547441244125366\n",
      "2150 1.3208849430084229 1.6374635696411133\n",
      "2160 1.3835793733596802 1.608301043510437\n",
      "2170 1.3799023628234863 1.6629737615585327\n",
      "2180 1.3809088468551636 1.6188791990280151\n",
      "2190 1.3537068367004395 1.550905466079712\n",
      "2200 1.3567577600479126 1.629583716392517\n",
      "2210 1.3449976444244385 1.547579050064087\n",
      "2220 1.3535089492797852 1.5468971729278564\n",
      "2230 1.3589370250701904 1.5492095947265625\n",
      "2240 1.3331247568130493 1.5415217876434326\n",
      "2250 1.3410894870758057 1.473815679550171\n",
      "2260 1.3383235931396484 1.5270670652389526\n",
      "2270 1.374191164970398 1.6513283252716064\n",
      "2280 1.3335051536560059 1.6719300746917725\n",
      "2290 1.350756049156189 1.581402063369751\n",
      "2300 1.3631958961486816 1.5880320072174072\n",
      "2310 1.3507145643234253 1.6153124570846558\n",
      "2320 1.3477928638458252 1.5019726753234863\n",
      "2330 1.3418933153152466 1.6043541431427002\n",
      "2340 1.3422315120697021 1.5351673364639282\n",
      "2350 1.2672699689865112 1.5981181859970093\n",
      "2360 1.3480781316757202 1.583994746208191\n",
      "2370 1.3972705602645874 1.5432474613189697\n",
      "2380 1.313110113143921 1.5731818675994873\n",
      "2390 1.3034005165100098 1.4866299629211426\n",
      "2400 1.3037813901901245 1.5538828372955322\n",
      "2410 1.2884103059768677 1.569595217704773\n",
      "2420 1.3510054349899292 1.50471031665802\n",
      "2430 1.2971709966659546 1.62965989112854\n",
      "2440 1.2910802364349365 1.5769339799880981\n",
      "2450 1.3264617919921875 1.6670629978179932\n",
      "2460 1.3347582817077637 1.613143801689148\n",
      "2470 1.37269127368927 1.642648696899414\n",
      "2480 1.344639778137207 1.6401907205581665\n",
      "2490 1.3202954530715942 1.4982789754867554\n",
      "2500 1.2964589595794678 1.6559679508209229\n",
      "2510 1.2834208011627197 1.4685289859771729\n",
      "2520 1.4053150415420532 1.6193568706512451\n",
      "2530 1.3457727432250977 1.4658870697021484\n",
      "2540 1.312483549118042 1.5535364151000977\n",
      "2550 1.289965271949768 1.551102638244629\n",
      "2560 1.3071742057800293 1.4715123176574707\n",
      "2570 1.3370685577392578 1.6504583358764648\n",
      "2580 1.3128788471221924 1.5258737802505493\n",
      "2590 1.3198072910308838 1.5145338773727417\n",
      "2600 1.2566115856170654 1.547586441040039\n",
      "2610 1.3629636764526367 1.6012524366378784\n",
      "2620 1.283468246459961 1.5551247596740723\n",
      "2630 1.3299158811569214 1.5898668766021729\n",
      "2640 1.2830610275268555 1.4340900182724\n",
      "2650 1.3617404699325562 1.6233108043670654\n",
      "2660 1.2884842157363892 1.4935460090637207\n",
      "2670 1.3613086938858032 1.608156442642212\n",
      "2680 1.257386565208435 1.5779762268066406\n",
      "2690 1.3508708477020264 1.4768966436386108\n",
      "2700 1.3123308420181274 1.4819270372390747\n",
      "2710 1.3027818202972412 1.549573302268982\n",
      "2720 1.3145748376846313 1.5276460647583008\n",
      "2730 1.2363356351852417 1.5642095804214478\n",
      "2740 1.290109634399414 1.5841362476348877\n",
      "2750 1.3077514171600342 1.598745584487915\n",
      "2760 1.3011029958724976 1.6217365264892578\n",
      "2770 1.2763211727142334 1.558349370956421\n",
      "2780 1.313652753829956 1.4876328706741333\n",
      "2790 1.2789783477783203 1.5240572690963745\n",
      "2800 1.301669716835022 1.5913255214691162\n",
      "2810 1.2681554555892944 1.6392443180084229\n",
      "2820 1.290004014968872 1.640728235244751\n",
      "2830 1.2801532745361328 1.5401314496994019\n",
      "2840 1.3307076692581177 1.5645829439163208\n",
      "2850 1.3036937713623047 1.5224173069000244\n",
      "2860 1.282461166381836 1.5636405944824219\n",
      "2870 1.3021098375320435 1.6501108407974243\n",
      "2880 1.2997887134552002 1.519072413444519\n",
      "2890 1.3217202425003052 1.5452711582183838\n",
      "2900 1.3301012516021729 1.4924724102020264\n",
      "2910 1.332329273223877 1.5130573511123657\n",
      "2920 1.2258636951446533 1.5611361265182495\n",
      "2930 1.24776291847229 1.5112420320510864\n",
      "2940 1.280258059501648 1.5351139307022095\n",
      "2950 1.2958661317825317 1.5196079015731812\n",
      "2960 1.2306783199310303 1.5615266561508179\n",
      "2970 1.2607154846191406 1.5593252182006836\n",
      "2980 1.3006702661514282 1.5345304012298584\n",
      "2990 1.3061898946762085 1.5678914785385132\n",
      "3000 1.3061611652374268 1.6428556442260742\n",
      "3010 1.285990834236145 1.452555775642395\n",
      "3020 1.3036302328109741 1.5616624355316162\n",
      "3030 1.269500732421875 1.527076244354248\n",
      "3040 1.2911031246185303 1.5744214057922363\n",
      "3050 1.2167112827301025 1.5646777153015137\n",
      "3060 1.2674949169158936 1.5540926456451416\n",
      "3070 1.2521709203720093 1.5571671724319458\n",
      "3080 1.2827829122543335 1.518410086631775\n",
      "3090 1.2529739141464233 1.5505003929138184\n",
      "3100 1.282831072807312 1.467248797416687\n",
      "3110 1.2816640138626099 1.4706767797470093\n",
      "3120 1.221994161605835 1.4577877521514893\n",
      "3130 1.3433772325515747 1.4849811792373657\n",
      "3140 1.2621780633926392 1.5491276979446411\n",
      "3150 1.3087193965911865 1.5746618509292603\n",
      "3160 1.2757105827331543 1.520256519317627\n",
      "3170 1.2475593090057373 1.5712463855743408\n",
      "3180 1.2673735618591309 1.5544092655181885\n",
      "3190 1.2569822072982788 1.5750898122787476\n",
      "3200 1.2551714181900024 1.4887290000915527\n",
      "3210 1.2480945587158203 1.5272011756896973\n",
      "3220 1.273511290550232 1.6142328977584839\n",
      "3230 1.2986007928848267 1.560647964477539\n",
      "3240 1.277368426322937 1.4984995126724243\n",
      "3250 1.2738980054855347 1.5119118690490723\n",
      "3260 1.3098030090332031 1.5029141902923584\n",
      "3270 1.3080919981002808 1.525390863418579\n",
      "3280 1.2883198261260986 1.534637451171875\n",
      "3290 1.3198459148406982 1.4962748289108276\n",
      "3300 1.2881027460098267 1.5834590196609497\n",
      "3310 1.2935292720794678 1.569185495376587\n",
      "3320 1.2273600101470947 1.643896222114563\n",
      "3330 1.2928946018218994 1.5281578302383423\n",
      "3340 1.2441047430038452 1.4348318576812744\n",
      "3350 1.2434303760528564 1.5468645095825195\n",
      "3360 1.2325456142425537 1.6008683443069458\n",
      "3370 1.2586690187454224 1.5084867477416992\n",
      "3380 1.2042804956436157 1.49699866771698\n",
      "3390 1.2673943042755127 1.5446453094482422\n",
      "3400 1.2399790287017822 1.5185816287994385\n",
      "3410 1.280317783355713 1.6237107515335083\n",
      "3420 1.2606573104858398 1.5258045196533203\n",
      "3430 1.1974077224731445 1.5403873920440674\n",
      "3440 1.2097899913787842 1.562432885169983\n",
      "3450 1.2683534622192383 1.5143946409225464\n",
      "3460 1.258907675743103 1.4970033168792725\n",
      "3470 1.2363123893737793 1.527160406112671\n",
      "3480 1.236397385597229 1.5466901063919067\n",
      "3490 1.2481412887573242 1.6544158458709717\n",
      "3500 1.2693653106689453 1.5260028839111328\n",
      "3510 1.26816725730896 1.5380939245224\n",
      "3520 1.2624378204345703 1.5392194986343384\n",
      "3530 1.2347297668457031 1.6089528799057007\n",
      "3540 1.2634767293930054 1.5492385625839233\n",
      "3550 1.2913941144943237 1.625281572341919\n",
      "3560 1.2604947090148926 1.546689510345459\n",
      "3570 1.257177472114563 1.5342763662338257\n",
      "3580 1.277040719985962 1.567682147026062\n",
      "3590 1.2225799560546875 1.5235340595245361\n",
      "3600 1.2205322980880737 1.5710251331329346\n",
      "3610 1.2348411083221436 1.5712776184082031\n",
      "3620 1.2879489660263062 1.4507193565368652\n",
      "3630 1.2835770845413208 1.5973074436187744\n",
      "3640 1.1999720335006714 1.3815065622329712\n",
      "3650 1.23369300365448 1.4900695085525513\n",
      "3660 1.267457365989685 1.4942001104354858\n",
      "3670 1.2484878301620483 1.5860750675201416\n",
      "3680 1.2015790939331055 1.4567015171051025\n",
      "3690 1.2841472625732422 1.52933669090271\n",
      "3700 1.2632737159729004 1.5888004302978516\n",
      "3710 1.1979374885559082 1.5898964405059814\n",
      "3720 1.2632598876953125 1.4804831743240356\n",
      "3730 1.2210599184036255 1.5482332706451416\n",
      "3740 1.2594572305679321 1.5217669010162354\n",
      "3750 1.244063377380371 1.5319586992263794\n",
      "3760 1.2731107473373413 1.4642956256866455\n",
      "3770 1.2993345260620117 1.4997549057006836\n",
      "3780 1.2634203433990479 1.5287981033325195\n",
      "3790 1.2182979583740234 1.5518643856048584\n",
      "3800 1.2396271228790283 1.5145585536956787\n",
      "3810 1.2415188550949097 1.5240623950958252\n",
      "3820 1.1967971324920654 1.4909571409225464\n",
      "3830 1.235619306564331 1.6489014625549316\n",
      "3840 1.2160210609436035 1.5419113636016846\n",
      "3850 1.2656806707382202 1.5326699018478394\n",
      "3860 1.2228268384933472 1.4467107057571411\n",
      "3870 1.2458422183990479 1.4463403224945068\n",
      "3880 1.1992731094360352 1.4537785053253174\n",
      "3890 1.2455378770828247 1.4788038730621338\n",
      "3900 1.2409528493881226 1.5392951965332031\n",
      "3910 1.2302173376083374 1.4470055103302002\n",
      "3920 1.1728007793426514 1.5248252153396606\n",
      "3930 1.2322413921356201 1.570393681526184\n",
      "3940 1.179566740989685 1.5075207948684692\n",
      "3950 1.2640269994735718 1.529226303100586\n",
      "3960 1.2100424766540527 1.5436307191848755\n",
      "3970 1.208929181098938 1.5146031379699707\n",
      "3980 1.2131251096725464 1.5433191061019897\n",
      "3990 1.2237187623977661 1.5164333581924438\n",
      "4000 1.2065067291259766 1.55437433719635\n",
      "4010 1.256162166595459 1.5415071249008179\n",
      "4020 1.2356860637664795 1.5746911764144897\n",
      "4030 1.2296245098114014 1.517885446548462\n",
      "4040 1.201365351676941 1.5252104997634888\n",
      "4050 1.2008298635482788 1.5816874504089355\n",
      "4060 1.2209289073944092 1.5818202495574951\n",
      "4070 1.1531282663345337 1.56344473361969\n",
      "4080 1.2151031494140625 1.5686111450195312\n",
      "4090 1.2132593393325806 1.5472826957702637\n",
      "4100 1.266479253768921 1.5393298864364624\n",
      "4110 1.2076709270477295 1.5339561700820923\n",
      "4120 1.2248260974884033 1.5352286100387573\n",
      "4130 1.2568695545196533 1.542905569076538\n",
      "4140 1.222491979598999 1.5193674564361572\n",
      "4150 1.2098031044006348 1.5493894815444946\n",
      "4160 1.2083828449249268 1.5510300397872925\n",
      "4170 1.1754218339920044 1.5820804834365845\n",
      "4180 1.2098960876464844 1.5491106510162354\n",
      "4190 1.2305980920791626 1.552055835723877\n",
      "4200 1.23446524143219 1.5321810245513916\n",
      "4210 1.2187774181365967 1.484022855758667\n",
      "4220 1.1443854570388794 1.569195032119751\n",
      "4230 1.2062660455703735 1.539905309677124\n",
      "4240 1.2370485067367554 1.5704326629638672\n",
      "4250 1.212878942489624 1.5497868061065674\n",
      "4260 1.2347846031188965 1.535341739654541\n",
      "4270 1.2415728569030762 1.5903748273849487\n",
      "4280 1.1603933572769165 1.615868330001831\n",
      "4290 1.2297440767288208 1.5737868547439575\n",
      "4300 1.2270939350128174 1.5501863956451416\n",
      "4310 1.2676652669906616 1.513381838798523\n",
      "4320 1.2428127527236938 1.586843729019165\n",
      "4330 1.197645902633667 1.4494295120239258\n",
      "4340 1.258080244064331 1.4432955980300903\n",
      "4350 1.253963828086853 1.5260640382766724\n",
      "4360 1.1796760559082031 1.528362512588501\n",
      "4370 1.1871891021728516 1.5461249351501465\n",
      "4380 1.2009897232055664 1.4476886987686157\n",
      "4390 1.2274854183197021 1.5345324277877808\n",
      "4400 1.2138787508010864 1.5751864910125732\n",
      "4410 1.1826670169830322 1.4721094369888306\n",
      "4420 1.2098053693771362 1.4535343647003174\n",
      "4430 1.1920934915542603 1.4807833433151245\n",
      "4440 1.1853790283203125 1.5221741199493408\n",
      "4450 1.2322642803192139 1.4943516254425049\n",
      "4460 1.2394624948501587 1.5981693267822266\n",
      "4470 1.1892136335372925 1.4993386268615723\n",
      "4480 1.2513753175735474 1.4565045833587646\n",
      "4490 1.2057828903198242 1.5151950120925903\n",
      "4500 1.2142225503921509 1.5378174781799316\n",
      "4510 1.118825912475586 1.4385504722595215\n",
      "4520 1.182466983795166 1.480210542678833\n",
      "4530 1.2165443897247314 1.6221160888671875\n",
      "4540 1.1892503499984741 1.5108343362808228\n",
      "4550 1.246438980102539 1.4431134462356567\n",
      "4560 1.2188329696655273 1.5756535530090332\n",
      "4570 1.1836742162704468 1.5088144540786743\n",
      "4580 1.1321357488632202 1.5688533782958984\n",
      "4590 1.1758708953857422 1.572190523147583\n",
      "4600 1.2057682275772095 1.5256500244140625\n",
      "4610 1.1653343439102173 1.558266282081604\n",
      "4620 1.2261463403701782 1.5570261478424072\n",
      "4630 1.2231879234313965 1.542077898979187\n",
      "4640 1.2024383544921875 1.5354986190795898\n",
      "4650 1.2154394388198853 1.5762238502502441\n",
      "4660 1.1978822946548462 1.545656681060791\n",
      "4670 1.1918752193450928 1.5212726593017578\n",
      "4680 1.2063896656036377 1.5006426572799683\n",
      "4690 1.169291377067566 1.5240229368209839\n",
      "4700 1.1703819036483765 1.5022717714309692\n",
      "4710 1.1526422500610352 1.5152703523635864\n",
      "4720 1.1677864789962769 1.4959187507629395\n",
      "4730 1.184916615486145 1.6006921529769897\n",
      "4740 1.1613150835037231 1.481001377105713\n",
      "4750 1.2230042219161987 1.4768867492675781\n",
      "4760 1.1891560554504395 1.4653444290161133\n",
      "4770 1.1767559051513672 1.5624024868011475\n",
      "4780 1.188555121421814 1.640684723854065\n",
      "4790 1.2185386419296265 1.5410423278808594\n",
      "4800 1.1645538806915283 1.4774470329284668\n",
      "4810 1.2027664184570312 1.5041022300720215\n",
      "4820 1.2165042161941528 1.557015061378479\n",
      "4830 1.2093441486358643 1.483300805091858\n",
      "4840 1.1718275547027588 1.555163860321045\n",
      "4850 1.2262247800827026 1.5526419878005981\n",
      "4860 1.1905238628387451 1.516474962234497\n",
      "4870 1.2011240720748901 1.5183188915252686\n",
      "4880 1.1593735218048096 1.4888336658477783\n",
      "4890 1.1833189725875854 1.4055767059326172\n",
      "4900 1.1596670150756836 1.6686701774597168\n",
      "4910 1.1708292961120605 1.5361888408660889\n",
      "4920 1.1607519388198853 1.4756133556365967\n",
      "4930 1.17638099193573 1.5438920259475708\n",
      "4940 1.1921207904815674 1.6246391534805298\n",
      "4950 1.174852728843689 1.4073940515518188\n",
      "4960 1.1816816329956055 1.4951586723327637\n",
      "4970 1.139025092124939 1.562039852142334\n",
      "4980 1.1661978960037231 1.612857699394226\n",
      "4990 1.2089040279388428 1.5779417753219604\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T11:01:48.929730Z",
     "start_time": "2025-12-05T11:01:48.849474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#saving the model state and hyperparameters\n",
    "checkpoint = {\"model_state_dict\": m.state_dict(),\n",
    "              \"config\": {\"vocab_size\": vocab_size,\n",
    "                         \"n_embd\": n_embd,\n",
    "                         \"n_layer\": n_layer,\n",
    "                         \"n_heads\": n_heads,\n",
    "                         \"dropout\": dropout,\n",
    "                         \"block_size\": block_size,\n",
    "                         \"learning_rate\": learning_rate,\n",
    "                         }}\n",
    "\n",
    "torch.save(checkpoint, 'bigram_model.pth')"
   ],
   "id": "7a04563971343b30",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T10:40:08.098267Z",
     "start_time": "2025-12-05T10:39:54.396676Z"
    }
   },
   "cell_type": "code",
   "source": "print(decode(m.generate(idx, max_new_tokens=1000)[0].tolist()))",
   "id": "2a239e9608f722d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First officers; let it set me rod\n",
      "with our great, a good chamber;\n",
      "Or, rille one of feal, her nor birth.\n",
      "\n",
      "Son:\n",
      "Concern more we your living tere for worse.\n",
      "\n",
      "CATESBY:\n",
      "For all they are corrupt,\n",
      "I'll not be some stir, sir.\n",
      "Lord Hastings, thou dess of marriage\n",
      "And will that Master Froth comestragem.\n",
      "\n",
      "PAULINA:\n",
      "Go to!\n",
      "The queen, lords;\n",
      "What late is muster'd true, will straight my Belty,\n",
      "Standing bad and releign, and by like a word?\n",
      "\n",
      "KING RICHARD II:\n",
      "Nay, troath, but that to so out outch?\n",
      "\n",
      "BUCKINGHAM:\n",
      "Lord his masters, who is't to my mother will,\n",
      "Than this is he did obey his voot: therefore\n",
      "His doubt out. Proclaims 'here's:\n",
      "I in an apped again is gone.\n",
      "\n",
      "CAPULET:\n",
      "And Thomas I trust our content;\n",
      "And, with a scepter, how stripe, first,\n",
      "Thy graven's doubles of death, that the dold,\n",
      "All debrive with the thren room times shound\n",
      "As Rodder's master's die spice.' thy thought\n",
      "I ransoming that thou, look your moath. Your tenths,\n",
      "Even are not i' death with spirit, to wing\n",
      "A did charge him to praise in't; b\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T10:40:21.952196Z",
     "start_time": "2025-12-05T10:40:08.183699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#write the generated text to a file\n",
    "generated_text = decode(m.generate(idx, max_new_tokens=1000)[0].tolist())\n",
    "with open('generated_text.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(generated_text)\n"
   ],
   "id": "9d6f251eefa6fa06",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T11:03:03.708580Z",
     "start_time": "2025-12-05T11:02:54.953746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#load the model.\n",
    "\n",
    "model = BigramLanguageModel().to(device)\n",
    "config = checkpoint['config']\n",
    "vocab_size = config['vocab_size']\n",
    "n_embd = config['n_embd']\n",
    "n_layer = config['n_layer']\n",
    "n_heads = config['n_heads']\n",
    "dropout = config['dropout']\n",
    "block_size = config['block_size']\n",
    "learning_rate = config['learning_rate']\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "m = model\n",
    "for i in range(3):\n",
    "    print(f\"iteration number {i + 1}:\")\n",
    "    print(decode(m.generate(idx, max_new_tokens=300)[0].tolist()))\n",
    "    print(\"\\n\\n\")"
   ],
   "id": "52e2bf81a57793bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number 1:\n",
      "\n",
      "JULIET:\n",
      "That is to reprieve.\n",
      "\n",
      "ROMEO:\n",
      "O thou never faction be none;\n",
      "Would ungon this world I promise myself?\n",
      "\n",
      "Nurse:\n",
      "The town it, the valiant planet, of all this\n",
      "silence will War justice: he says the law, Master\n",
      "too. What! must I not not of this? I have been drinked a note of this?\n",
      "\n",
      "First Murderer:\n",
      "A\n",
      "\n",
      "\n",
      "\n",
      "iteration number 2:\n",
      "\n",
      "HORTENSIO:\n",
      "Be but conruly be daughter.\n",
      "\n",
      "KING HENRY VI:\n",
      "Have your sight shall be so like a poor indeed,\n",
      "Could not be since repred no pride heir fur best,\n",
      "But will you speak me a word: you well conceive\n",
      "It rage like a town: you are disinherited,\n",
      "And make his life so confessor.\n",
      "\n",
      "Lady:\n",
      "My lord,\n",
      "Which is\n",
      "\n",
      "\n",
      "\n",
      "iteration number 3:\n",
      "\n",
      "MONTAGUE:\n",
      "Some chamber, and yet be loved:\n",
      "Let them tell thee there had sometime forth,\n",
      "Thou love a great general of Rome;\n",
      "Or else praise-have was venom to fight our care.\n",
      "Dost thou not hear it, and put up thy nose,\n",
      "Which, which first hich doth nurse in the ease.\n",
      "\n",
      "ROMEO:\n",
      "I love it did not saw Warwick\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 42
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
